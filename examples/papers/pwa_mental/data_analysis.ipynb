{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Finger on the Pulse of Happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and data transformations\n",
    "import matplotlib.pyplot as plt  # For plotting graphs and visualizations\n",
    "import seaborn as sns  # For advanced data visualization (using seaborn built on top of matplotlib)\n",
    "# import utilities as util  # Assuming this is a custom module for additional utility functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import shap\n",
    "import sys\n",
    "import os\n",
    "import psychai.statistics.data_preparation.normalization as norm\n",
    "import psychai.statistics.data_preparation.outlier as outlier\n",
    "import psychai.statistics.linear_regression.linear_regression as lr\n",
    "import psychai.machine_learning.regression_supervise_learning as rsl\n",
    "# import psychai.statistics.bland_altman_analysis.bland_altman_analysis as baa\n",
    "\n",
    "do_correlation_analysis = True  # Flag to enable correlation analysis\n",
    "do_regression_analysis = True  # Flag to enable regression analysis\n",
    "do_machine_learning = True  # Flag to enable machine learning tasks\n",
    "do_stepwise_regression = False  # Flag to enable stepwise regression analysis\n",
    "do_categorical_machine_learning = True  # Flag to enable categorical ML analysis\n",
    "try_transformation = True  # Flag to enable transformation of variables (for model improvement or scaling)\n",
    "plot_significant_only = False  # Plot only significant features/variables\n",
    "plot_all = False  # Plot all variables regardless of significance\n",
    "print_summary = True  # Print a summary of the results or analysis\n",
    "do_processed_output = True  # Output processed data\n",
    "do_descriptive_statistics = True  # Calculate and print descriptive statistics\n",
    "\n",
    "root_file_path = os.getcwd()\n",
    "\n",
    "# ---- Constants ----\n",
    "# These are fixed values that define thresholds, limits, and parameters for the analysis.\n",
    "\n",
    "# Total number of combinations for some kind of data analysis or model training\n",
    "num_combinations = 14 * 23 * 3\n",
    "\n",
    "# Adjust the significance level using Bonferroni correction (to control for multiple comparisons)\n",
    "# This ensures more stringent criteria when considering a variable as statistically significant.\n",
    "significant_level = 0.05 / num_combinations  \n",
    "\n",
    "# Upper bound for model validation threshold\n",
    "model_valid_upper_bound = 2\n",
    "\n",
    "# The minimum percentage of valid waveform data required (for quality control purposes)\n",
    "valid_waveform_percentage_lower_bound = 0.8  # Originally set at 0.8 (can be adjusted)\n",
    "\n",
    "# Minimum data points per heartbeat for valid signal analysis (ensuring data quality)\n",
    "data_points_per_heartbeat_lower_bound = 15  # Originally set at 15\n",
    "\n",
    "# Minimum length of PPG (Photoplethysmogram) data for a valid analysis\n",
    "ppg_length_lower_bound = 100\n",
    "\n",
    "# Upper limit for the age of participants (ensuring that analysis is within a reasonable age range)\n",
    "age_upper_bound = 100\n",
    "\n",
    "# Gender filter (for models or analysis that differentiate by gender)\n",
    "# Can be \"M\" for male, \"F\" for female, or \"Both\" to include all genders in the analysis.\n",
    "gender = \"Both\"  \n",
    "\n",
    "# Outlier multiplier (used to detect and remove extreme values in data, typically in boxplot-based analysis)\n",
    "k = 1.5  # Common multiplier for detecting outliers using the IQR method\n",
    "\n",
    "save_path = os.path.join(root_file_path,\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data\n",
    "def read_data(file_name):\n",
    "    full_path = os.path.join(root_file_path,\"data\",file_name)\n",
    "    return pd.read_csv(full_path, sep=\"\\t\")\n",
    "\n",
    "# Read data\n",
    "data = read_data(\"data.txt\")\n",
    "data.columns = data.columns.str.replace('.', '_', regex=False)\n",
    "print(len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data Filtering ----\n",
    "\n",
    "# Drop rows where any of the key columns contain NaN values (i.e., missing data).\n",
    "# This ensures we have complete data for essential variables used in the analysis.\n",
    "# Columns dropped if NaN: 'Valid_Model_Percentage', 'ReflectionIndex', 'ValidWaveformPercentage',\n",
    "# 'PPG_Heart_Rate', 'SWLS', 'The_Subjective_Vitality_Scale', 'GAD_7', 'PHQ_9', 'PANAS_P'\n",
    "data_complete = data.dropna(subset=[\n",
    "    'Valid_Model_Percentage', 'ReflectionIndex', 'ValidWaveformPercentage', \n",
    "    'PPG_Heart_Rate', 'SWLS', 'The_Subjective_Vitality_Scale', \n",
    "    'GAD_7', 'PHQ_9', 'PANAS_P'\n",
    "])\n",
    "\n",
    "# ---- Creating Gender-Specific Datasets ----\n",
    "# Create separate datasets for males (M) and females (F) based on the 'Gender' column.\n",
    "# This allows for gender-specific analysis later on.\n",
    "\n",
    "# Filter the data to include only male participants\n",
    "data_complete_m = data_complete[data_complete['Gender'] == \"M\"]\n",
    "\n",
    "# Filter the data to include only female participants\n",
    "data_complete_f = data_complete[data_complete['Gender'] == \"F\"]\n",
    "\n",
    "# ---- Calculating Average Age and Standard Deviation for Each Gender ----\n",
    "# This provides summary statistics of the participants' age by gender for comparison.\n",
    "\n",
    "# Calculate the mean age of male participants\n",
    "complete_average_age_m = data_complete_m['Age'].mean()\n",
    "\n",
    "# Calculate the mean age of female participants\n",
    "complete_average_age_f = data_complete_f['Age'].mean()\n",
    "\n",
    "# Calculate the standard deviation of age for male participants (a measure of age variation)\n",
    "complete_std_age_m = data_complete_m['Age'].std()\n",
    "\n",
    "# Calculate the standard deviation of age for female participants\n",
    "complete_std_age_f = data_complete_f['Age'].std()\n",
    "\n",
    "# ---- Further Data Filtering ----\n",
    "\n",
    "# Create a copy of the complete dataset to apply further filtering based on quality control measures.\n",
    "data_valid = data_complete.copy()\n",
    "\n",
    "# 1. Filter out rows where the PPG (Photoplethysmogram) length is below the threshold.\n",
    "# This ensures that only participants with enough data points are considered for further analysis.\n",
    "data_valid = data_valid[data_valid['PPG_Length'] > ppg_length_lower_bound]\n",
    "\n",
    "# 2. Filter out rows where the number of data points per heartbeat is below the lower bound.\n",
    "# This ensures valid heartbeat measurements for each participant.\n",
    "data_valid = data_valid[data_valid['Data_Points_Per_Heartbeat'] > data_points_per_heartbeat_lower_bound]\n",
    "\n",
    "# 3. Filter out rows where the percentage of valid waveforms is below the specified threshold.\n",
    "# This quality control step ensures that only high-quality waveform data is used.\n",
    "data_valid = data_valid[data_valid['ValidWaveformPercentage'] > valid_waveform_percentage_lower_bound]\n",
    "\n",
    "# ---- Removing Rows with Missing PSD (Power Spectral Density) Values ----\n",
    "# The 'psd_columns' list contains the names of the columns for PSD values.\n",
    "# These columns must not have NaN values (i.e., missing data), so we drop any rows where one or more are missing.\n",
    "# PSD is often used in analyzing signals (e.g., heart rate, PPG), so complete data is essential for this analysis.\n",
    "\n",
    "psd_columns = ['PSD_0', 'PSD_1', 'PSD_2', 'PSD_3', 'PSD_4', 'PSD_5', 'PSD_6']\n",
    "\n",
    "# Drop rows where any of the PSD columns contain NaN values.\n",
    "data_valid = data_valid.dropna(subset=psd_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "data_valid['HR'] = data_valid['HR_Average']\n",
    "data_valid['Vitality'] = data_valid['The_Subjective_Vitality_Scale'] \n",
    "\n",
    "# rename\n",
    "data_valid.rename(columns = {'ReflectionIndex':'RI'}, inplace = True) \n",
    "data_valid.rename(columns = {'EstimatedReflectionIndex':'E_RI'}, inplace = True) \n",
    "data_valid.rename(columns = {'AdaptedReflectionIndex':'A_RI'}, inplace = True) \n",
    "\n",
    "# duplicate\n",
    "data_valid['PPT'] = data_valid['FirstPeakToSecondPeakTime'] \n",
    "data_valid['CT'] = data_valid['ValleyToFirstPeakTime'] \n",
    "\n",
    "# Calculating various other parameters\n",
    "\n",
    "data_valid['NT'] = data_valid['ValleyToFirstPeakTime'] + data_valid['FirstPeakToNotchTime']\n",
    "data_valid['DT'] = data_valid['FirstPeakToSecondPeakTime'] + data_valid['SecondPeakToValleyTime']\n",
    "data_valid['A1'] = data_valid['FirstPeakHeight'] * data_valid['ValleyToFirstPeakTime'] / 2 + (data_valid['FirstPeakHeight'] + data_valid['NotchHeight']) * data_valid['FirstPeakToNotchTime'] / 2\n",
    "data_valid['A2'] = data_valid['NotchHeight'] * data_valid['NotchToValleyTime'] / 2\n",
    "data_valid['IPA'] = data_valid['A2'] / data_valid['A1']\n",
    "data_valid['RCA'] = data_valid['ValleyToFirstPeakTime'] / data_valid['NT']\n",
    "data_valid['RDA'] = data_valid['FirstPeakToNotchTime'] / (data_valid['ValleyToFirstPeakTime'] + data_valid['FirstPeakToSecondPeakTime'] + data_valid['SecondPeakToValleyTime'])\n",
    "\n",
    "data_valid['SI'] = data_valid['Height'] / data_valid['FirstPeakToSecondPeakTime']\n",
    "\n",
    "# Calculating curvature features\n",
    "data_valid['B_A'] = abs(data_valid['FirstNegativeSecondDerivative']/data_valid['FirstPositiveSecondDerivative'])\n",
    "data_valid['E_A'] = abs(data_valid['SecondPositiveSecondDerivative']/data_valid['FirstPositiveSecondDerivative'])\n",
    "data_valid['F_A'] = abs(data_valid['SecondNegativeSecondDerivative']/data_valid['FirstPositiveSecondDerivative'])\n",
    "data_valid['G_A'] = abs(data_valid['ThirdPositiveSecondDerivative']/data_valid['FirstPositiveSecondDerivative'])\n",
    "data_valid['H_A'] = abs(data_valid['ThirdNegativeSecondDerivative']/data_valid['FirstPositiveSecondDerivative'])\n",
    "data_valid = data_valid.drop('C_A', axis=1)\n",
    "data_valid = data_valid.drop('D_A', axis=1)\n",
    "\n",
    "# Calculating PSD and ratios\n",
    "psd_cols = ['PSD_1', 'PSD_2', 'PSD_3', 'PSD_4', 'PSD_5', 'PSD_6']\n",
    "data_valid['PSD_All'] = data_valid[psd_cols].sum(axis=1)\n",
    "for col in psd_cols:\n",
    "    data_valid[f'r_{col}'] = data_valid[col] / data_valid['PSD_All']\n",
    "\n",
    "data_valid['NHA'] = 1 - data_valid['PSD_1']**2 / data_valid[psd_cols].pow(2).sum(axis=1)\n",
    "data_valid['IHAR'] = (1 - data_valid['NHA']) / data_valid['IPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'data_valid' is your DataFrame\n",
    "normalized_by_ppg = ['B_A', 'E_A', 'F_A','G_A', 'H_A', \"RI\", \"E_RI\", \"A_RI\", \"PPT\", \"SI\", \"CT\", \"NT\", \"DT\", \"IPA\", \"RCA\", \"RDA\", \"r_PSD_1\", \"r_PSD_2\", \"r_PSD_3\", \"r_PSD_4\", \"r_PSD_5\", \"r_PSD_6\", \"NHA\", \"IHAR\"]\n",
    "\n",
    "# Normalize by PPG and HR\n",
    "data_valid = norm.normalize_and_add_by_variables(data_valid, normalized_by_ppg, \"PPG_Heart_Rate\", 75, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Aging Index and normalize it\n",
    "data_valid['Aging_Index'] = np.abs(data_valid['B_A']) - np.abs(data_valid['E_A'])\n",
    "data_valid, _ = norm.normalize_and_add_by_variable(data_valid, 'Aging_Index', 'PPG_Heart_Rate', 75)\n",
    "\n",
    "# Apply logarithmic transformations\n",
    "for col in ['B_A_75', 'E_A_75', 'F_A_75','G_A_75', 'H_A_75']:\n",
    "    log_col_name = f'{col}_Log'\n",
    "    data_valid[log_col_name] = np.log(data_valid[col] - data_valid[col].min() + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable lists\n",
    "power_spectrum_densities_2_75 = [\"r_PSD_1_75\", \"r_PSD_2_75\", \"r_PSD_3_75\", \"r_PSD_4_75\", \"r_PSD_5_75\", \"r_PSD_6_75\"]\n",
    "acceleration_75 = [\"B_A_75\", \"E_A_75\", \"F_A_75\", \"G_A_75\", \"H_A_75\", \"NHA_75\", \"IHAR_75\"]\n",
    "waveform_75 = [\"RI_75\", \"E_RI_75\", \"A_RI_75\", \"PPT_75\", \"SI_75\", \"Aging_Index_75\", \"CT_75\", \"NT_75\", \"DT_75\", \"IPA_75\", \"RCA_75\", \"RDA_75\"]\n",
    "normalized_by_height = waveform_75 + acceleration_75 + power_spectrum_densities_2_75\n",
    "\n",
    "# Normalize the dataset\n",
    "data_valid = norm.normalize_and_add_by_variables(data_valid, normalized_by_height, \"Height\", 170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to apply logarithmic transformation\n",
    "log_transform_variables = [\n",
    "    'B_A', 'G_A', 'H_A', 'E_A', 'F_A'\n",
    "]\n",
    "\n",
    "# Apply logarithmic transformation and append as new columns\n",
    "for var in log_transform_variables:\n",
    "    transformed_var = f'{var}_Log'\n",
    "    data_valid[transformed_var] = np.log(data_valid[var] - data_valid[var].min() + 0.1)\n",
    "\n",
    "\n",
    "# List of PSD variables to apply logarithmic transformation\n",
    "psd_variables = [\n",
    "    'r_PSD_1', 'r_PSD_2', 'r_PSD_3',\n",
    "    'r_PSD_4', 'r_PSD_5', 'r_PSD_6'\n",
    "]\n",
    "\n",
    "# Apply logarithmic transformation and append as new columns\n",
    "for var in psd_variables:\n",
    "    transformed_var = f'{var}_Log'\n",
    "    data_valid[transformed_var] = np.log(data_valid[var] - data_valid[var].min() + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply logarithmic transformation and append as new columns\n",
    "data_valid['NHA_Log'] = np.log(data_valid['NHA'] - data_valid['NHA'].min() + 0.1)\n",
    "data_valid['IHAR_Log'] = np.log(data_valid['IHAR'] - data_valid['IHAR'].min() + 0.1)\n",
    "data_valid['SI_75_Log'] = np.log(data_valid['SI_75'] - data_valid['SI_75'].min() + 0.1)\n",
    "data_valid['SI_75_170_Log'] = np.log(data_valid['SI_75_170'] - data_valid['SI_75_170'].min() + 0.1)\n",
    "\n",
    "# Save the DataFrame to a file\n",
    "data_valid.to_csv(os.path.join(save_path,\"Data-1-Valid.txt\"), sep=\"\\t\", index=False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_valid' is a pandas DataFrame\n",
    "# and 'normalized_to_dataset' is defined as previously discussed\n",
    "\n",
    "# Defining variable lists\n",
    "pulse_waveform_variables = [\"FirstPeakToSecondPeakTime\", \"ReflectionIndex\", \"EstimatedReflectionIndex\", \"RI_75\", \"E_RI_75\", \"SI\", \"SI_75\"]\n",
    "power_spectrum_densities = [\"Relative_PSD_1\", \"Relative_PSD_2\", \"Relative_PSD_3\", \"Relative_PSD_4\", \"Relative_PSD_5\", \"Relative_PSD_6\"]\n",
    "power_spectrum_densities_2 = [\"r_PSD_1\", \"r_PSD_2\", \"r_PSD_3\", \"r_PSD_4\", \"r_PSD_5\", \"r_PSD_6\", \"NHA\", \"IHAR\"]\n",
    "power_spectrum_densities_75 = [\"Relative_PSD_1_75\", \"Relative_PSD_2_75\", \"Relative_PSD_3_75\", \"Relative_PSD_4_75\", \"Relative_PSD_5_75\", \"Relative_PSD_6_75\"]\n",
    "acceleration = [\"B_A\", \"C_A\", \"D_A\", \"E_A\", \"F_A\"]\n",
    "acceleration_75_Log = [\"B_A_75_Log\", \"C_A_75_Log\", \"D_A_75_Log\", \"E_A_75_Log\", \"F_A_75_Log\"]\n",
    "acceleration_75_170_Log = [\"B_A_75_170_Log\", \"C_A_75_170_Log\", \"D_A_75_170_Log\", \"E_A_75_170_Log\", \"F_A_75_170_Log\"]\n",
    "HRV = [\"rMSSD_1\", \"pNN50_1\", \"SDNN_1\", \"HF1_1\", \"LF1_1\"]\n",
    "\n",
    "# Normalize HRV variables\n",
    "data_valid = norm.normalize_and_add_by_variables(data_valid, HRV, \"PPG_Heart_Rate\", 75)\n",
    "\n",
    "# Defining variable lists\n",
    "controlled_variables = [\"Height\", \"HR\"]\n",
    "psychological_variables = [\"PHQ_9\", \"GAD_7\", \"UCLA\", \"PANAS_P\", \"PANAS_N\", \"SWLS\",\"Vitality\", \"Extraversion\", \"Agreeableness\", \"Conscientiousness\", \"Stability\", \"Openness\",  \"V\", \"A\"]\n",
    "dependent_variables = psychological_variables  # Assuming psychological_variables is already defined\n",
    "independent_variables_raw = [\"PPT\", \"CT\", \"IPA\", \"RCA\", \"RDA\", \"SI\", \"RI\", \"E_RI\", \"B_A\", \"E_A\", \"F_A\", \"G_A\", \"H_A\", \"Aging_Index\"] + power_spectrum_densities_2\n",
    "independent_variables_75 = [\"PPT_75\", \"CT_75\", \"DT_75\", \"NT_75\", \"IPA_75\", \"RCA_75\", \"RDA_75\", \"SI_75\", \"RI_75\", \"E_RI_75\", \"Aging_Index_75\"] + acceleration_75 + power_spectrum_densities_2_75\n",
    "power_spectrum_densities_2_75_170_height = [\"r_PSD_1_75_170\", \"r_PSD_2_75_170\", \"r_PSD_3_75_170\", \"r_PSD_4_75_170\", \"r_PSD_5_75_170\", \"r_PSD_6_75_170\"]\n",
    "power_spectrum_densities_2_75_170_height_Log = [\"r_PSD_1_75_170_Log\", \"r_PSD_2_75_170_Log\", \"r_PSD_3_75_170_Log\", \"r_PSD_4_75_170_Log\", \"r_PSD_5_75_170_Log\", \"r_PSD_6_75_170_Log\", \"NHA_75_170_Log\", \"IHAR_75_170_Log\"]\n",
    "acceleration_75_170_height = [\"B_A_75_170\", \"E_A_75_170\", \"F_A_75_170\",\"G_A_75_170\", \"H_A_75_170\",  \"NHA_75_170\", \"IHAR_75_170\"]\n",
    "waveform_75_170_height = [\"RI_75_170\", \"E_RI_75_170\", \"A_RI_75_170\", \"PPT_75_170\", \"Aging_Index_75_170\", \"CT_75_170\", \"NT_75_170\", \"DT_75_170\", \"IPA_75_170\", \"RCA_75_170\", \"RDA_75_170\"]\n",
    "independent_variables = [\"SI_75\"]  + waveform_75_170_height + acceleration_75_170_height + power_spectrum_densities_2_75_170_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descpritive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display descriptive statistics for participants\n",
    "def get_descriptive_data(df_input):\n",
    "    \"\"\"\n",
    "    This function calculates and prints basic descriptive statistics for participants in a dataset.\n",
    "    It handles unique participants, gender distribution, and age statistics.\n",
    "    \n",
    "    Args:\n",
    "        df_input (DataFrame): The input DataFrame containing participant data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure each participant appears only once by removing duplicate entries based on 'Participant_SN'.\n",
    "    unique_participants = df_input.drop_duplicates(subset='Participant_SN')\n",
    "    \n",
    "    # Print the total number of unique participants.\n",
    "    num_participants = len(unique_participants)\n",
    "    print(f\"Number of participants: {num_participants}\")\n",
    "\n",
    "    # ---- Gender Statistics ----\n",
    "    \n",
    "    # Calculate the number of male participants.\n",
    "    male_participants = len(unique_participants[unique_participants['Gender'] == 'M'])\n",
    "    \n",
    "    # Calculate the percentage of male participants.\n",
    "    male_percentage = (male_participants / num_participants) * 100\n",
    "    \n",
    "    # Print the percentage of male participants.\n",
    "    print(f\"Percentage of male participants: {male_percentage:.2f}%\")\n",
    "    \n",
    "    # ---- Age Statistics ----\n",
    "    \n",
    "    # Calculate and print the average (mean) age of the participants.\n",
    "    average_age = unique_participants['Age'].mean()\n",
    "    print(f\"Average age: {average_age:.2f}\")\n",
    "    \n",
    "    # Calculate and print the standard deviation of age (measuring variability in participant ages).\n",
    "    age_std = unique_participants['Age'].std()\n",
    "    print(f\"Standard deviation of age: {age_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_descriptive_data(data)\n",
    "print('====')\n",
    "get_descriptive_data(data_complete)\n",
    "print('====')\n",
    "get_descriptive_data(data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_descriptive_statistics:\n",
    "    # Defining the list of variables for descriptive statistics\n",
    "    descriptive_statistics = [\"Height\", \"Age\", \"Gender_1\"] + controlled_variables + dependent_variables + independent_variables\n",
    "\n",
    "    # Grouping data by 'Participant_SN' and calculating the mean for each variable\n",
    "    descriptive_statistics_table = data_valid.groupby('Participant_SN')[descriptive_statistics].mean().reset_index()\n",
    "\n",
    "    # Renaming 'Participant_SN' column\n",
    "    descriptive_statistics_table.rename(columns={'Participant_SN': 'ParticipantSN'}, inplace=True)\n",
    "\n",
    "    # Save the descriptive statistics table to a file\n",
    "    descriptive_statistics_table.to_csv(os.path.join(save_path,\"Description_Analysis.txt\"), sep=\",\", index=False)\n",
    "\n",
    "    # Print results\n",
    "    print(descriptive_statistics_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 16\n",
    "if do_processed_output:\n",
    "    # Starting with ID and ValidWaveformPercentage\n",
    "    data_output = data_valid[['ID', 'ValidWaveformPercentage']]\n",
    "\n",
    "    # Adding columns from various lists\n",
    "    for var_list in [independent_variables, dependent_variables, controlled_variables, psychological_variables]:\n",
    "        for var in var_list:\n",
    "            if var in data_valid.columns:\n",
    "                data_output[var] = data_valid[var]\n",
    "\n",
    "    # Save the output DataFrame to a file\n",
    "    \n",
    "    data_output.to_csv(os.path.join(save_path,\"processed_data_median_Valid_Larger_Than_0.8.csv\"), sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font size for the chart elements\n",
    "font_size = 10\n",
    "\n",
    "# List of variable names\n",
    "independent_variables = [\n",
    "    'E_RI_75_170',\n",
    "    'SI_75',\n",
    "    'CT_75_170',\n",
    "    'IPA_75_170',\n",
    "    'RCA_75_170',\n",
    "    'PPT_75_170',\n",
    "    'Aging_Index_75_170',\n",
    "    'NT_75_170',\n",
    "    'DT_75_170',\n",
    "    'RDA_75_170',\n",
    "    'B_A_75_170',\n",
    "    'E_A_75_170',\n",
    "    'F_A_75_170',\n",
    "    'G_A_75_170',\n",
    "    'H_A_75_170',\n",
    "    'r_PSD_1_75_170',\n",
    "    'r_PSD_2_75_170',\n",
    "    'r_PSD_3_75_170',\n",
    "    'r_PSD_4_75_170',\n",
    "    'r_PSD_5_75_170',\n",
    "    'r_PSD_6_75_170',\n",
    "    'NHA_75_170',\n",
    "    'IHAR_75_170',\n",
    "]\n",
    "\n",
    "# Corresponding display names for the chart\n",
    "display_names = [\n",
    "    'RI',\n",
    "    'SI',\n",
    "    'CT',\n",
    "    'IPA',\n",
    "    'RCA',\n",
    "    'PPT',\n",
    "    'AI',\n",
    "    'NT',\n",
    "    'DT',\n",
    "    'RDA',\n",
    "    'B/A',\n",
    "    'E/A',\n",
    "    'F/A',\n",
    "    'G/A',\n",
    "    'H/A',\n",
    "    'PSD1',\n",
    "    'PSD2',\n",
    "    'PSD3',\n",
    "    'PSD4',\n",
    "    'PSD5',\n",
    "    'PSD6',\n",
    "    'NHA',\n",
    "    'IHAR',\n",
    "]\n",
    "\n",
    "if do_correlation_analysis:\n",
    "    mean_sd_table = []\n",
    "    all_variables = independent_variables\n",
    "\n",
    "    for i, var in enumerate(all_variables):\n",
    "        display_name = display_names[i]\n",
    "        with_outliers = data_valid[var]\n",
    "        without_outliers = outlier.remove_outliers(data_valid, var, k)[var]\n",
    "\n",
    "        with_outlier_stats = [display_name, with_outliers.mean(), with_outliers.std(), len(with_outliers)]\n",
    "        without_outlier_stats = [without_outliers.mean(), without_outliers.std(), len(without_outliers)]\n",
    "\n",
    "        mean_sd_table.append(with_outlier_stats + without_outlier_stats)\n",
    "\n",
    "    columns = [\"Variable\", \"WithOutlierMean\", \"WithOutlierSD\", \"WithOutlierN\", \"WithoutOutlierMean\", \"WithoutOutlierSD\", \"WithoutOutlierN\"]\n",
    "    mean_sd_df = pd.DataFrame(mean_sd_table, columns=columns)\n",
    "\n",
    "    # Calculating correlation matrix\n",
    "    data_cor_table = data_valid[all_variables].corr(method=\"pearson\")\n",
    "\n",
    "    # Replace variable names with display names for correlation matrix\n",
    "    data_cor_table.columns = display_names\n",
    "    data_cor_table.index = display_names\n",
    "\n",
    "    # Resetting index to avoid issues\n",
    "    data_cor_table.reset_index(drop=True, inplace=True)\n",
    "    mean_sd_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create a list of numbers for the x-axis\n",
    "    x_labels = list(range(1, len(display_names) + 1))\n",
    "\n",
    "    # Custom annotation function to remove leading zero for 0.xx and convert 1.00 to 1\n",
    "    def custom_annot(val):\n",
    "        if val == 1.00:\n",
    "            return '1'\n",
    "        else:\n",
    "            return f'{val:.2f}'.replace(\"0.\", \".\")\n",
    "\n",
    "    # Displaying the correlation matrix heatmap with annotations\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Create the heatmap with font size adjustments\n",
    "    ax = sns.heatmap(data_cor_table, cmap='coolwarm', center=0, linewidths=.5, annot=True, fmt=\".2f\",\n",
    "                     annot_kws={\"size\": font_size},  # Annotation font size\n",
    "                     yticklabels=display_names, xticklabels=x_labels, cbar_kws={'label': 'Correlation'})\n",
    "\n",
    "    # Apply custom formatting to the annotations\n",
    "    for t in ax.texts:\n",
    "        t.set_text(custom_annot(float(t.get_text())))\n",
    "\n",
    "    # Set font size for the title and labels\n",
    "    plt.xlabel('Variable Number', fontsize=font_size)\n",
    "    plt.ylabel('Variable Name', fontsize=font_size)\n",
    "    plt.title('Correlation Coefficient Matrix', fontsize=font_size)\n",
    "\n",
    "    # Set font size for the ticks (x and y axis labels)\n",
    "    plt.xticks(fontsize=font_size)\n",
    "    plt.yticks(fontsize=font_size)\n",
    "\n",
    "    # Adjust font size for the color bar (legend bar)\n",
    "    colorbar = ax.collections[0].colorbar\n",
    "    colorbar.ax.tick_params(labelsize=font_size)  # Set the font size of the colorbar ticks\n",
    "    colorbar.set_label('Correlation', fontsize=font_size)  # Set the font size of the colorbar label\n",
    "\n",
    "    # Display the heatmap with annotations\n",
    "    plt.show()\n",
    "\n",
    "    # Concatenating the correlation matrix and the mean-SD DataFrame\n",
    "    save_result = pd.concat([data_cor_table, mean_sd_df], axis=1)\n",
    "\n",
    "    # Saving the result to a file\n",
    "    full_path = os.path.join(save_path,\"correlation_matrix.txt\")\n",
    "    save_result.to_csv(full_path, sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_regression_analysis:\n",
    "    result_df, largest_r_squared_df = lr.explore_simple_linear_regression_all(data_valid, dependent_variables, independent_variables, k, try_transformation, plot_all, plot_significant_only, print_summary, significant_level)\n",
    "\n",
    "    # Save the results to a file\n",
    "    result_df.to_csv(os.path.join(save_path,\"Result.txt\"), sep=\"\\t\", index=False)\n",
    "    largest_r_squared_df.to_csv(os.path.join(save_path,\"largest_r_squared_Result.txt\"), sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_methods = [\"Random Forest\"]\n",
    "dependent_variables = psychological_variables\n",
    "data_valid_name_limited = data_valid.copy()\n",
    "data_valid_name_replaced = data_valid_name_limited[independent_variables+dependent_variables]\n",
    "data_valid_name_replaced.rename(columns=dict(zip(independent_variables, display_names)), inplace=True)\n",
    "\n",
    "if do_machine_learning:\n",
    "    rsl.regression_supervised_learning(data_valid_name_replaced, prediction_methods,dependent_variables,display_names,(50,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "- Copyright：Ivan Liu.\n",
    "- Last updated: 2024/10/11\n",
    "- Anaconda Environment：course_4\n",
    "- Reference:\n",
    "    - None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

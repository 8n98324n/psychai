{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Thermal Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime\n",
    "import subprocess\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Get the parent directory\n",
    "# This is useful if you need to import modules from a parent directory.\n",
    "parent_dir = os.path.abspath('..')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import any custom utilities from the parent directory (if needed)\n",
    "import utilities\n",
    "\n",
    "data_preprocessing = utilities.DataPreProcessing()\n",
    "# Log file to track processed folders and steps\n",
    "PROCESSED_LOG = \"../results/facial/process_log_disk_6.json\"\n",
    "# PROCESSED_LOG = \"../results/thermal/process_log_local.json\"\n",
    "\n",
    "main_folder = r'K:\\Backup\\Experiments\\Moral Elevation\\Disk1_2_combined\\FS-9 Day 1'\n",
    "# main_folder = r'D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\raw_data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Preparation\n",
    "\n",
    "The following code use files in folder \"recorded_video\" to segment videos from \"thermal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "This module contains classes and methods for transforming a BVP signal in a BPM signal.\n",
    "\"\"\"\n",
    "\n",
    "class Path:\n",
    "    \"\"\"\n",
    "    Manage (multi-channel, row-wise) BVP signals, and transforms them in BPMs.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def check_path_or_create(self, path):\n",
    "        # Check if the path exists\n",
    "        if not os.path.exists(path):\n",
    "            # If it does not exist, create the directory\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "    \n",
    "# Step 1: Parse the \"Time\" folder file names to extract the time information\n",
    "def parse_time_file(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    user_number = parts[1]\n",
    "    step = int(parts[2])\n",
    "    month = int(parts[3])\n",
    "    day = int(parts[4])\n",
    "    end_hour = int(parts[5])\n",
    "    end_minute = int(parts[6])\n",
    "    end_second = int(parts[7].replace('.mp4', ''))\n",
    "\n",
    "    return {\n",
    "        'file_name': file_name,\n",
    "        'user_number': user_number,\n",
    "        'step': step,\n",
    "        'month': month,\n",
    "        'day': day,\n",
    "        'hour': end_hour,\n",
    "        'minute': end_minute,\n",
    "        'second': end_second,\n",
    "        'start_time': datetime.datetime(2022, month, day, end_hour, end_minute, end_second)\n",
    "    }\n",
    "\n",
    "def parse_thermal_file(file_path):\n",
    "    try:\n",
    "        # Extract the file name and split it into parts\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_name = file_name.replace('T', '_')\n",
    "        parts = file_name.split('_')\n",
    "        \n",
    "        # Validate if the expected parts are in the file name\n",
    "        if len(parts) < 3:\n",
    "            raise ValueError(f\"Filename format is incorrect: {file_name}\")\n",
    "\n",
    "        date = parts[1]\n",
    "        time = parts[2].replace('.mp4', '')\n",
    "\n",
    "        # Validate the date and time format\n",
    "        if len(date) != 8 or len(time) != 6:\n",
    "            raise ValueError(f\"Date or time format in filename is incorrect: {file_name}\")\n",
    "\n",
    "        # Extract and convert date and time components from the filename\n",
    "        year = int(date[:4])\n",
    "        month = int(date[4:6])\n",
    "        day = int(date[6:8])\n",
    "        hour = int(time[:2])  # 12-hour format from filename\n",
    "        minute = int(time[2:4])\n",
    "        second = int(time[4:6])\n",
    "\n",
    "        # Get the modified time of the file to determine AM or PM\n",
    "        modified_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "        \n",
    "        # Adjust hour based on modified time's 24-hour format as a reference\n",
    "        if modified_time.hour >= 12 and hour < 12:\n",
    "            hour += 12  # Convert PM hour to 24-hour format\n",
    "        elif modified_time.hour < 12 and hour == 12:\n",
    "            hour = 0  # Handle 12 AM case\n",
    "\n",
    "        # Return parsed start time if successful\n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'start_time': datetime.datetime(year, month, day, hour, minute, second)\n",
    "        }\n",
    "\n",
    "    except (IndexError, ValueError) as e:\n",
    "        print(f\"Error parsing file: {file_name}. Details: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and update processed log\n",
    "def load_and_update_processed_log():\n",
    "    # Load log data if it exists; otherwise, initialize an empty dictionary\n",
    "    if os.path.exists(PROCESSED_LOG):\n",
    "        with open(PROCESSED_LOG, 'r') as file:\n",
    "            log_data = json.load(file)\n",
    "    else:\n",
    "        log_data = {}\n",
    "\n",
    "    # Nested function to save the log\n",
    "    def save_log():\n",
    "        with open(PROCESSED_LOG, 'w') as file:\n",
    "            json.dump(log_data, file, indent=4)\n",
    "\n",
    "    return log_data, save_log\n",
    "    \n",
    "def segment_video_ffmpeg(input_video_path, start_time_seconds, end_time_seconds, output_video_path):\n",
    "    command = [\n",
    "        'ffmpeg', '-y',\n",
    "        '-ss', str(start_time_seconds),\n",
    "        '-i', input_video_path,\n",
    "        '-t', str(end_time_seconds - start_time_seconds),\n",
    "        '-c', 'copy',\n",
    "        output_video_path\n",
    "    ]\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error in FFmpeg command:\", result.stderr.decode())\n",
    "    else:\n",
    "        print(\"Video segment created successfully:\", output_video_path)\n",
    "\n",
    "def rotate_if_horizontal_ffmpeg(thermal_file_path, input_video_path, output_video_path):\n",
    "    # Open the video and get dimensions\n",
    "    video_capture = cv2.VideoCapture(thermal_file_path)\n",
    "    width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Read the first frame for brightness analysis\n",
    "    success, frame = video_capture.read()\n",
    "    video_capture.release()\n",
    "\n",
    "    if success:\n",
    "        # Check if the video is horizontal\n",
    "        is_horizontal = width > height\n",
    "\n",
    "        if is_horizontal:\n",
    "            # For horizontal videos, take upper and lower 10% strips\n",
    "            upper_strip = frame[:height // 10, :]\n",
    "            lower_strip = frame[-height // 10:, :]\n",
    "\n",
    "            # Split each strip into left and right halves\n",
    "            upper_left_half = upper_strip[:, :width // 4]\n",
    "            upper_right_half = upper_strip[:, width // 4:]\n",
    "            lower_left_half = lower_strip[:, :width // 4]\n",
    "            lower_right_half = lower_strip[:, width // 4:]\n",
    "\n",
    "            # Calculate the average brightness of each half for both strips\n",
    "            upper_left_brightness = np.mean(cv2.cvtColor(upper_left_half, cv2.COLOR_BGR2GRAY))\n",
    "            upper_right_brightness = np.mean(cv2.cvtColor(upper_right_half, cv2.COLOR_BGR2GRAY))\n",
    "            lower_left_brightness = np.mean(cv2.cvtColor(lower_left_half, cv2.COLOR_BGR2GRAY))\n",
    "            lower_right_brightness = np.mean(cv2.cvtColor(lower_right_half, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "            # Average brightness for left and right sides across both strips\n",
    "            left_brightness = (upper_left_brightness + lower_left_brightness) / 2\n",
    "            right_brightness = (upper_right_brightness + lower_right_brightness) / 2\n",
    "\n",
    "            # # Determine rotation direction based on brightness\n",
    "            if right_brightness > left_brightness:\n",
    "                # Rotate 90 degrees clockwise\n",
    "                print(f\"Rotating {input_video_path} 90 degrees clockwise...\")\n",
    "                #The error message indicates that FFmpeg encountered an issue with buffering the audio stream. The error \"Too many packets buffered for output stream 0:1\" often arises when FFmpeg tries to handle both video and audio streams, especially with formats that have low frame rates or non-standard encoding parameters.\n",
    "                #Therefore Remove the Audio Stream\n",
    "                command = [\n",
    "                    'ffmpeg', '-y', '-i', input_video_path, '-vf', 'transpose=1',\n",
    "                    '-c:v', 'libx264', '-an', output_video_path\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                # Rotate 90 degrees counterclockwise\n",
    "                print(f\"Rotating {input_video_path} 90 degrees counterclockwise...\")\n",
    "                command = [\n",
    "                    'ffmpeg', '-y', '-i', input_video_path, '-vf', 'transpose=2',\n",
    "                    '-c:v', 'libx264', '-an', output_video_path\n",
    "                ]\n",
    "\n",
    "            # Run FFmpeg command and capture output\n",
    "            result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            \n",
    "            # Print debug information and check for errors\n",
    "            if result.returncode != 0:\n",
    "                print(\"FFmpeg error:\", result.stderr.decode())\n",
    "            else:\n",
    "                print(\"Rotation successful. Output saved to:\", output_video_path)\n",
    "\n",
    "            # Print FFmpeg output for additional debugging (optional)\n",
    "            print(\"FFmpeg output:\", result.stdout.decode())\n",
    "        else:\n",
    "            # For vertical videos, take left and right 10% strips\n",
    "            left_strip = frame[:, :width // 10]\n",
    "            right_strip = frame[:, -width // 10:]\n",
    "\n",
    "            # Split each strip into upper and lower halves\n",
    "            left_upper_half = left_strip[:height // 2, :]\n",
    "            left_lower_half = left_strip[height // 2:, :]\n",
    "            right_upper_half = right_strip[:height // 2, :]\n",
    "            right_lower_half = right_strip[height // 2:, :]\n",
    "\n",
    "            # Calculate the average brightness of each half for both strips\n",
    "            left_upper_brightness = np.mean(cv2.cvtColor(left_upper_half, cv2.COLOR_BGR2GRAY))\n",
    "            left_lower_brightness = np.mean(cv2.cvtColor(left_lower_half, cv2.COLOR_BGR2GRAY))\n",
    "            right_upper_brightness = np.mean(cv2.cvtColor(right_upper_half, cv2.COLOR_BGR2GRAY))\n",
    "            right_lower_brightness = np.mean(cv2.cvtColor(right_lower_half, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "            # Average brightness of upper and lower halves from both strips\n",
    "            upper_brightness = (left_upper_brightness + right_upper_brightness) / 2\n",
    "            lower_brightness = (left_lower_brightness + right_lower_brightness) / 2\n",
    "\n",
    "            # Rotate 180 degrees if the upper half is lighter than the lower half\n",
    "            if upper_brightness > lower_brightness:\n",
    "                print(f\"Rotating {input_video_path} 180 degrees as upper half is lighter...\")\n",
    "                command = [\n",
    "                    'ffmpeg', '-y', '-i', input_video_path, '-vf', 'vflip,hflip',\n",
    "                    '-c:v', 'libx264', '-an', output_video_path\n",
    "                ]\n",
    "                subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            else:\n",
    "                print(f\"Keeping {input_video_path} intact as upper half is darker.\")\n",
    "                command = [\n",
    "                    'ffmpeg', '-y', '-i', input_video_path, '-c:v', 'copy',\n",
    "                    '-c:v', 'libx264', '-an', output_video_path\n",
    "                ]\n",
    "                subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "            # Adding basic error handling\n",
    "            result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            if result.returncode != 0:\n",
    "                print(\"FFmpeg error:\", result.stderr.decode())\n",
    "            else:\n",
    "                print(\"Operation completed successfully. Output saved to:\", output_video_path)\n",
    "\n",
    "# Function to concatenate two videos using ffmpeg\n",
    "def concatenate_videos(video1_path, video2_path, output_path):\n",
    "    with open('concat_list.txt', 'w') as f:\n",
    "        f.write(f\"file '{video1_path}'\\n\")\n",
    "        f.write(f\"file '{video2_path}'\\n\")\n",
    "    \n",
    "    command = [\n",
    "        'ffmpeg', '-y', '-f', 'concat', '-safe', '0',\n",
    "        '-i', 'concat_list.txt', '-c', 'copy', output_path\n",
    "    ]\n",
    "    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    os.remove('concat_list.txt')\n",
    "\n",
    "# Function to process each step\n",
    "def process_step(step, thermal_info, thermal_file_path, fps, duration, segmented_folder, user_number):\n",
    "    step_id = str(step['step'])\n",
    "    output_filename = f\"PS-9_{user_number}_{step['step']}_{step['month']:02d}_{step['day']:02d}_{step['hour']:02d}_{step['minute']:02d}_{step['second']:02d}.mp4\"\n",
    "    output_path = os.path.join(segmented_folder, output_filename)\n",
    "\n",
    "    video_start_time = thermal_info['start_time']\n",
    "    segment_start_seconds = max((step['start_time'] - video_start_time).total_seconds(), 0)\n",
    "    segment_end_seconds = min((step['end_time'] - video_start_time).total_seconds(), duration)\n",
    "\n",
    "    if segment_start_seconds >= duration or segment_end_seconds <= 0:\n",
    "        #print(f\"Step {step['step']} does not overlap with the video time.\")\n",
    "        return None\n",
    "\n",
    "    if segment_end_seconds - segment_start_seconds < 1:\n",
    "        #print(f\"The file {output_filename} is too short to save.\")\n",
    "        return None\n",
    "\n",
    "    # Temporary file before rotation or concatenation\n",
    "    temp_output_path = os.path.join(segmented_folder, \"temp_\" + output_filename)\n",
    "\n",
    "    # Segment the video\n",
    "    segment_video_ffmpeg(thermal_file_path, segment_start_seconds, segment_end_seconds, temp_output_path)\n",
    "\n",
    "    # Temp file for the rotated video\n",
    "    temp_rotated_output_path = os.path.join(segmented_folder, \"rotated_\" + output_filename)\n",
    "\n",
    "    # Rotate the video if needed\n",
    "    rotate_if_horizontal_ffmpeg(thermal_file_path, temp_output_path, temp_rotated_output_path)\n",
    "\n",
    "    # Append the video if the output file already exists\n",
    "    # if os.path.exists(output_path):\n",
    "    #     temp_concat_output = os.path.join(segmented_folder, \"concat_\" + output_filename)\n",
    "    #     if os.path.exists(temp_rotated_output_path):\n",
    "    #         concatenate_videos(output_path, temp_rotated_output_path, temp_concat_output)\n",
    "    #         os.replace(temp_concat_output, output_path)\n",
    "    if os.path.exists(output_path):\n",
    "        # Open the video file to get fps and duration\n",
    "        video_capture = cv2.VideoCapture(output_path)\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # If fps is zero, release and return as it would cause division error\n",
    "        if fps == 0:\n",
    "            video_capture.release()\n",
    "            return\n",
    "        \n",
    "        # Calculate the duration\n",
    "        duration = frame_count / fps\n",
    "        video_capture.release()\n",
    "\n",
    "        # Define the concatenated output path\n",
    "        temp_concat_output = os.path.join(segmented_folder, \"concat_\" + output_filename)\n",
    "\n",
    "        if duration > 1:\n",
    "            # If duration > 1 second and temp_rotated_output_path exists, concatenate the videos\n",
    "            if os.path.exists(temp_rotated_output_path):\n",
    "                concatenate_videos(output_path, temp_rotated_output_path, temp_concat_output)\n",
    "                if os.path.exists(temp_concat_output):\n",
    "                    os.replace(temp_concat_output, output_path)\n",
    "        else:\n",
    "            # If duration <= 1 second, replace output_path with temp_rotated_output_path\n",
    "            if os.path.exists(temp_rotated_output_path):\n",
    "                os.replace(temp_rotated_output_path, output_path)\n",
    "    else:\n",
    "        if os.path.exists(temp_rotated_output_path):\n",
    "            os.replace(temp_rotated_output_path, output_path)\n",
    "        elif os.path.exists(temp_output_path):\n",
    "            os.replace(temp_output_path, output_path)\n",
    "\n",
    "    # Clean up temporary files\n",
    "    if os.path.exists(temp_output_path):\n",
    "        os.remove(temp_output_path)\n",
    "    if os.path.exists(temp_rotated_output_path):\n",
    "        os.remove(temp_rotated_output_path)\n",
    "\n",
    "    print(f\"Processed and saved: {output_filename}\")\n",
    "    return step_id\n",
    "\n",
    "# Main function to segment the video based on steps and thermal info\n",
    "def segment_video(thermal_file_path, steps, segmented_folder, user_number, processed_log, step_numbers=None):\n",
    "    thermal_info = parse_thermal_file(thermal_file_path)\n",
    "\n",
    "    # Filter steps based on the provided list of step_numbers\n",
    "    if step_numbers is not None:\n",
    "        steps = [step for step in steps if step['step'] in step_numbers]\n",
    "        if not steps:\n",
    "            #print(f\"No matching steps found for step numbers: {step_numbers}\")\n",
    "            return\n",
    "\n",
    "    # Open the video file to get fps and duration\n",
    "    video_capture = cv2.VideoCapture(thermal_file_path)\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if fps == 0:\n",
    "        video_capture.release()\n",
    "        return\n",
    "    duration = frame_count / fps\n",
    "    thermal_info['end_time'] = thermal_info['start_time'] + datetime.timedelta(seconds=duration)\n",
    "\n",
    "    # Collect processed steps\n",
    "    processed_steps = []\n",
    "\n",
    "    for step in steps:\n",
    "        result = process_step(step, thermal_info, thermal_file_path, fps, duration, segmented_folder, user_number)\n",
    "        if result:\n",
    "            processed_steps.append(result)\n",
    "\n",
    "\n",
    "    # Update log with processed steps\n",
    "    folder_key = user_number\n",
    "    if folder_key not in processed_log:\n",
    "        processed_log[folder_key] = {}\n",
    "    for step_id in processed_steps:\n",
    "        processed_log[folder_key][step_id] = True\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "\n",
    "# Process each user folder\n",
    "def process_user_folder(user_folder, step_numbers=None):\n",
    "    # Load and prepare processed log and save function\n",
    "    processed_log, save_log = load_and_update_processed_log()\n",
    "\n",
    "    # Check if this user folder is already in the log\n",
    "    folder_key = os.path.basename(user_folder)\n",
    "    if folder_key not in processed_log:\n",
    "        processed_log[folder_key] = {}\n",
    "\n",
    "    time_folder = os.path.join(user_folder, 'RecordedVideo')\n",
    "    thermal_folder = os.path.join(user_folder, 'thermal')\n",
    "\n",
    "    # Ensure both folders exist for the user\n",
    "    if not os.path.exists(time_folder) or not os.path.exists(thermal_folder):\n",
    "        return\n",
    "\n",
    "    user_number = os.path.basename(user_folder)\n",
    "    segmented_folder = os.path.join(user_folder, 'Segmented_Thermal')\n",
    "    if not os.path.exists(segmented_folder):\n",
    "        os.makedirs(segmented_folder)\n",
    "\n",
    "    time_files = sorted([f for f in os.listdir(time_folder) if f.endswith('.mp4')], key=lambda f: parse_time_file(f)['step'])\n",
    "    thermal_files = sorted([f for f in os.listdir(thermal_folder) if f.endswith('.mp4')])\n",
    "\n",
    "    # Parse the time files and determine the begin and end times for each step\n",
    "    steps = []\n",
    "    for time_file in time_files:\n",
    "        time_info = parse_time_file(time_file)\n",
    "        step_id = str(time_info['step'])\n",
    "\n",
    "        # Check if this step has already been processed for this user\n",
    "        if step_id in processed_log[folder_key]:\n",
    "            # Check duration of the time file\n",
    "            time_file_path = os.path.join(time_folder, time_file)\n",
    "            video_capture = cv2.VideoCapture(time_file_path)\n",
    "            frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "            video_duration = frame_count / fps if fps > 0 else 0\n",
    "            video_capture.release()\n",
    "            \n",
    "            # Check the segmented file's duration\n",
    "            segmented_file_path = os.path.join(segmented_folder, time_file)\n",
    "            segmented_capture = cv2.VideoCapture(segmented_file_path)\n",
    "            segmented_frame_count = int(segmented_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            segmented_fps = segmented_capture.get(cv2.CAP_PROP_FPS)\n",
    "            segmented_duration = segmented_frame_count / segmented_fps if segmented_fps > 0 else 0\n",
    "            segmented_capture.release()\n",
    "\n",
    "            # Reprocess if duration difference > 10% or either duration is 0\n",
    "            duration_difference = abs(video_duration - segmented_duration)\n",
    "            if (video_duration == 0 or segmented_duration == 0) or (duration_difference / video_duration > 0.1):\n",
    "                print(f\"Duration mismatch for step {step_id}. Reprocessing...\")\n",
    "            else:\n",
    "                print(f\"Step {step_id} for user {user_number} has already been processed. Skipping...\")\n",
    "                continue\n",
    "\n",
    "        # Process and determine start and end times\n",
    "        video_capture = cv2.VideoCapture(os.path.join(time_folder, time_file))\n",
    "        frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        if fps == 0:\n",
    "            continue\n",
    "        \n",
    "        video_duration = frame_count / fps\n",
    "        video_capture.release()\n",
    "\n",
    "        # Begin time is now determined by subtracting the video duration from the end time\n",
    "        time_info['end_time'] = time_info['start_time'] + datetime.timedelta(seconds=video_duration)\n",
    "        steps.append(time_info)\n",
    "\n",
    "    # Sort thermal files by extracting the timestamp information from the file names\n",
    "    try:\n",
    "        thermal_files = sorted(thermal_files, key=lambda f: parse_thermal_file(os.path.join(thermal_folder, f))['start_time'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error sorting thermal files: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Segment all thermal videos for the user\n",
    "    for thermal_file in thermal_files:\n",
    "        thermal_file_path = os.path.join(thermal_folder, thermal_file)\n",
    "        segment_video(thermal_file_path, steps, segmented_folder, user_number, processed_log, step_numbers)\n",
    "\n",
    "    # Save log after all processing is completed\n",
    "    save_log()\n",
    "    print(f\"Folder {user_folder} processed and recorded in log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from rich.progress import track\n",
    "\n",
    "# Define the main processing function\n",
    "def process_all_folders(main_folder, user_folders_to_process, step_numbers=[23], parallel=True):\n",
    "    def process_single_folder(user_folder):\n",
    "        try:\n",
    "            user_folder_path = os.path.join(main_folder, user_folder)\n",
    "            process_user_folder(user_folder_path, step_numbers=step_numbers)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing folder {user_folder}: {e}\")\n",
    "\n",
    "    if parallel:\n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            results = list(track(\n",
    "                executor.map(process_single_folder, user_folders_to_process),\n",
    "                total=len(user_folders_to_process),\n",
    "                description=\"Processing Folders\"\n",
    "            ))\n",
    "    else:\n",
    "        # Sequential processing with progress tracking\n",
    "        for user_folder in track(user_folders_to_process, description=\"Processing Folders\"):\n",
    "            process_single_folder(user_folder)\n",
    "\n",
    "# List of user numbers to process (e.g., [5, 6, 7]). If empty, process all user folders.\n",
    "user_numbers_to_process = [12,14,31,35,41,45,47,59,62,65,73,87,100,106] #list(range(1, 150))\n",
    "\n",
    "# Orignal Thermal File is Missing:  14, 31, 35, 41,45,47,59, 62, 65, 73, 87, 100, 106\n",
    "# Check if the user_numbers_to_process list is empty\n",
    "if user_numbers_to_process:\n",
    "    # Convert user numbers to folder names with zero-padding (e.g., '005', '006', '007')\n",
    "    user_folders_to_process = [f\"{num:03d}\" for num in user_numbers_to_process]\n",
    "else:\n",
    "    # If no specific user numbers are provided, process all available user folders in the main folder\n",
    "    user_folders_to_process = sorted(\n",
    "        [f for f in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, f))]\n",
    "    )\n",
    "\n",
    "\n",
    "process_all_folders(main_folder, user_folders_to_process, parallel=False)  # Set parallel to False for sequential processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Flipp Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "user_numbers_to_process = [11\n",
    "                            ,16\n",
    "                            ,20\n",
    "                            ,21\n",
    "                            ,22\n",
    "                            ,28\n",
    "                            ,29\n",
    "                            ,30\n",
    "                            ,36\n",
    "                            ,39\n",
    "                            ,40\n",
    "                            ,49\n",
    "                            ,51\n",
    "                            ,52\n",
    "                            ,53\n",
    "                            ,60\n",
    "                            ,61\n",
    "                            ,66\n",
    "                            ,67\n",
    "                            ,68\n",
    "                            ,72\n",
    "                            ,78\n",
    "                            ,81\n",
    "                            ,82\n",
    "                            ,83\n",
    "                            ,86\n",
    "                            ,94\n",
    "                            ,115]\n",
    "\n",
    "# Check if the user_numbers_to_process list is empty\n",
    "if user_numbers_to_process:\n",
    "    # Convert user numbers to folder names with zero-padding (e.g., '005', '006', '007')\n",
    "    user_folders_to_process = [f\"{num:03d}\" for num in user_numbers_to_process]\n",
    "\n",
    "for user_folder in user_folders_to_process:\n",
    "    segmented_thermal_folder = os.path.join(main_folder, user_folder, 'Segmented_Thermal')\n",
    "    segmented_thermal_files = []\n",
    "    if os.path.exists(segmented_thermal_folder):\n",
    "        segmented_thermal_files = sorted([f for f in os.listdir(segmented_thermal_folder) if f.endswith('.mp4')], key=lambda f: parse_time_file(f)['step'])\n",
    "\n",
    "    # Parse the time files and determine the begin and end times for each step\n",
    "    steps = []\n",
    "    for input_video_path in segmented_thermal_files:\n",
    "        output_video_path = \"rotated_\"+ input_video_path\n",
    "        full_output_video_path = os.path.join(main_folder, user_folder, 'Segmented_Thermal', output_video_path)\n",
    "        full_input_video_path = os.path.join(main_folder, user_folder, 'Segmented_Thermal', input_video_path)\n",
    "        command = [\n",
    "            'ffmpeg', '-y', '-i', full_input_video_path, '-vf', 'vflip,hflip',\n",
    "            '-c:v', 'libx264', '-an', full_output_video_path\n",
    "        ]\n",
    "        subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if os.path.exists(full_output_video_path):\n",
    "            os.replace(full_output_video_path, full_input_video_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(full_output_video_path):\n",
    "            os.remove(full_output_video_path)\n",
    "        \n",
    "        print(f\"{input_video_path} completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Rotate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "user_numbers_to_process = [6,7]\n",
    "\n",
    "# Check if the user_numbers_to_process list is empty\n",
    "if user_numbers_to_process:\n",
    "    # Convert user numbers to folder names with zero-padding (e.g., '005', '006', '007')\n",
    "    user_folders_to_process = [f\"{num:03d}\" for num in user_numbers_to_process]\n",
    "\n",
    "for user_folder in user_folders_to_process:\n",
    "    segmented_thermal_folder = os.path.join(main_folder, user_folder, 'Segmented_Thermal')\n",
    "    segmented_thermal_files = []\n",
    "    if os.path.exists(segmented_thermal_folder):\n",
    "        segmented_thermal_files = sorted([f for f in os.listdir(segmented_thermal_folder) if f.endswith('.mp4')], key=lambda f: parse_time_file(f)['step'])\n",
    "\n",
    "    # Parse the time files and determine the begin and end times for each step\n",
    "    steps = []\n",
    "    for input_video_path in segmented_thermal_files:\n",
    "        output_video_path = \"rotated_\"+ input_video_path\n",
    "        full_output_video_path = os.path.join(main_folder, user_folder, 'Segmented_Thermal', output_video_path)\n",
    "        full_input_video_path = os.path.join(main_folder, user_folder, 'Segmented_Thermal', input_video_path)\n",
    "        command = [\n",
    "            'ffmpeg', '-y', '-i', full_input_video_path, '-vf', 'transpose=1',\n",
    "            '-c:v', 'libx264', '-an', full_output_video_path\n",
    "        ]\n",
    "        subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if os.path.exists(full_output_video_path):\n",
    "            os.replace(full_output_video_path, full_input_video_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(full_output_video_path):\n",
    "            os.remove(full_output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"../resources/images/facial_roi.jpg\" width=\"200\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Video\n",
    "location = \"../resources/videos/thermal_participant_1.mp4\"\n",
    "Video(location, embed=False, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 如果想要旋转图像的话，见researchtoolbox.thermal_imaging.utils\n",
    "\n",
    "class features_extraction:\n",
    "    def __init__(self, model_path, input_file_path, output_file_path, para_dict):\n",
    "        cp = Path()\n",
    "        self.model_path = model_path\n",
    "        self.input_file_path = input_file_path\n",
    "        self.output_mp4_path = os.path.join(output_file_path,\"video_output\")\n",
    "        self.no_rectangle_image_path = os.path.join(output_file_path,\"test_images_output\")\n",
    "        self.visualization_path = os.path.join(output_file_path,\"visualization_output\")\n",
    "        self.output_csv_path =  os.path.join(output_file_path,\"csv_output\")\n",
    "\n",
    "        cp.check_path_or_create(self.model_path)\n",
    "        cp.check_path_or_create(self.input_file_path)\n",
    "        cp.check_path_or_create(self.output_mp4_path)\n",
    "        cp.check_path_or_create(self.no_rectangle_image_path)\n",
    "        cp.check_path_or_create(self.output_csv_path)\n",
    "        cp.check_path_or_create(self.visualization_path)\n",
    "\n",
    "        self.n_feature_points = para_dict.get(\"n_feature_points\", 54+7)\n",
    "        self.n_sampling = para_dict.get(\"n_sampling\", 30)\n",
    "        self.resize_height = para_dict.get(\"resize_height\", 600)\n",
    "        self.upsampling_times = para_dict.get(\"upsampling_times\", 1)\n",
    "        self.minimum_gray_color = para_dict.get(\"minimum_gray_color\", 30)\n",
    "\n",
    "        self.height_lower_threshold = np.int16(self.resize_height*0.2)\n",
    "        self.height_upper_threshold = np.int16(self.resize_height * 0.8)\n",
    "        self.adaptive_length = np.int16(30 * 10 / self.n_sampling)  # 多少禎以後，限制數值變化的範圍\n",
    "\n",
    "        # load the face detector (HOG-SVM)\n",
    "        print(\"[INFO] loading dlib thermal face detector...\")\n",
    "        self.detector = dlib.simple_object_detector(os.path.join(self.model_path, \"dlib_face_detector.svm\"))\n",
    "\n",
    "        # load the facial landmarks predictor\n",
    "        print(\"[INFO] loading facial landmark predictor...\")\n",
    "        self.predictor = dlib.shape_predictor(os.path.join(self.model_path, \"dlib_landmark_predictor.dat\"))\n",
    "\n",
    "    def folder_process(self):\n",
    "\n",
    "\n",
    "\n",
    "        video_files = list(paths.list_files(self.input_file_path))\n",
    "        # loop over the images\n",
    "        for ind, input_file_fullname in enumerate(video_files, 1):\n",
    "            print(\"[INFO] Processing video: {}/{}\".format(ind, len(video_files)))\n",
    "            self.file_process(input_file_fullname)\n",
    "\n",
    "\n",
    "\n",
    "        print('finished')\n",
    "\n",
    "    def file_process(self,input_file_fullname):\n",
    "        n_error = 0\n",
    "        file_name = os.path.splitext(os.path.basename(input_file_fullname))[0]\n",
    "        #output_file_fullname = self.output_mp4_path + \"output_\" + file_name + \".mp4\"\n",
    "        output_file_fullname = os.path.join(self.output_mp4_path,  \"output_\" + file_name + \".mp4\")\n",
    "        output_csv_file_fullname = os.path.join(self.output_csv_path, file_name + \".csv\")\n",
    "\n",
    "        # initialize the video stream\n",
    "        vs = cv2.VideoCapture(input_file_fullname)\n",
    "        length = int(vs.get(cv2. CAP_PROP_FRAME_COUNT))\n",
    "        # initialize the video writer\n",
    "        writer = None\n",
    "        (W, H) = (None, None)\n",
    "        gray_list = []\n",
    "        gray_list_for_chart = []\n",
    "        # 取得平均值 54+7維\n",
    "        average_list = np.zeros(self.n_feature_points)\n",
    "        # 禎數\n",
    "        n_frame = 0\n",
    "        n_grab = 0\n",
    "        # 取得平均寛度\n",
    "        rec_height_list = []\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        previous_gray_sub_list = np.zeros(self.n_feature_points)\n",
    "        while True:\n",
    "            #print(str(n_grab)+\" out of \"+str(length))\n",
    "            # read the next frame from the file\n",
    "            (grabbed, raw_frame) = vs.read()\n",
    "            n_grab = n_grab+1\n",
    "\n",
    "            # break the loop if the frame\n",
    "            # was not grabbed\n",
    "            if not grabbed:\n",
    "                break\n",
    "\n",
    "            if np.remainder(n_grab, self.n_sampling) != 0:  # 说明不是要抓取的帧\n",
    "                continue\n",
    "\n",
    "            # resize the frame\n",
    "            frame = imutils.resize(raw_frame, height=self.resize_height)\n",
    "\n",
    "            # copy the frame\n",
    "            frame_copy = frame.copy()\n",
    "\n",
    "            # convert the frame to grayscale\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2GRAY)\n",
    "\n",
    "            # detect faces in the frame\n",
    "            rects = self.detector(frame, upsample_num_times=self.upsampling_times)\n",
    "\n",
    "            if len(rec_height_list) == self.adaptive_length:\n",
    "                height_lower_threshold = np.median(rec_height_list[-1*self.adaptive_length:])*0.7\n",
    "                height_upper_threshold = np.median(rec_height_list[-1*self.adaptive_length:])*1.3\n",
    "                x_average = np.median(x_list[-1*self.adaptive_length:])\n",
    "                y_average = np.median(y_list[-1*self.adaptive_length:])\n",
    "\n",
    "            # loop over the bounding boxes\n",
    "            if len(rects) > 0:\n",
    "                for rect in rects:  # 抓取多张脸\n",
    "                    # if len(rects) >= 1:\n",
    "                    # rect = rects[0]  # 只抓取1张脸\n",
    "                    # convert the dlib rectangle into an OpenCV bounding box\n",
    "                    (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "\n",
    "                    if len(rec_height_list) < self.adaptive_length:\n",
    "                        y_average = y\n",
    "\n",
    "                    y_list.append(y)\n",
    "                    x_list.append(x)\n",
    "                    if self.height_lower_threshold < h < self.height_upper_threshold and np.abs(y-y_average)/y < 0.5*h:\n",
    "                        rec_height_list.append(h)  # 方框只在合規的範圍內調整\n",
    "                        try:\n",
    "                            # draw a bounding box surrounding the face\n",
    "                            cv2.rectangle(frame_copy, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                            # predict the location of facial landmark coordinates then\n",
    "                            # convert the prediction to an easily parsable NumPy array\n",
    "                            shape = self.predictor(frame, rect)\n",
    "                            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "                            # 左側臉頰加點\n",
    "                            point_4_x = shape[3][0]\n",
    "                            point_4_y = shape[3][1]\n",
    "                            point_32_x = shape[31][0]\n",
    "                            point_32_y = shape[31][1]\n",
    "                            point_left_cheek_x = np.int16(0.7*point_4_x+0.3*point_32_x)\n",
    "                            point_left_cheek_y = np.int16(0.7*point_4_y+0.3*point_32_y)\n",
    "                            shape = np.vstack([shape, [point_left_cheek_x, point_left_cheek_y]])\n",
    "                            point_left_nose_x = np.int16(0.2*point_4_x+0.8*point_32_x)\n",
    "                            point_left_nose_y = np.int16(0.2*point_4_y+0.8*point_32_y)\n",
    "                            shape = np.vstack([shape, [point_left_nose_x, point_left_nose_y]])\n",
    "                            # 右側臉頰加點\n",
    "                            point_14_x = shape[13][0]\n",
    "                            point_14_y = shape[13][1]\n",
    "                            point_36_x = shape[35][0]\n",
    "                            point_36_y = shape[35][1]\n",
    "                            point_right_cheek_x = np.int16(0.7*point_14_x+0.3*point_36_x)\n",
    "                            point_right_cheek_y = np.int16(0.7*point_14_y+0.3*point_36_y)\n",
    "                            shape = np.vstack([shape, [point_right_cheek_x, point_right_cheek_y]])\n",
    "                            point_right_nose_x = np.int16(0.2*point_14_x+0.8*point_36_x)\n",
    "                            point_right_nose_y = np.int16(0.2*point_14_y+0.8*point_36_y)\n",
    "                            shape = np.vstack([shape, [point_right_nose_x, point_right_nose_y]])\n",
    "                            # forehead\n",
    "                            point_22_x = shape[21][0]\n",
    "                            point_22_y = shape[21][1]\n",
    "                            point_23_x = shape[22][0]\n",
    "                            point_23_y = shape[22][1]\n",
    "                            point_28_x = shape[27][0]\n",
    "                            point_28_y = shape[27][1]\n",
    "                            point_fore_head_x = np.int16(point_22_x + point_23_x - point_28_x)\n",
    "                            point_fore_head_y = np.int16(0.75*point_22_y + 0.75*point_23_y - 0.5*point_28_y)\n",
    "                            shape = np.vstack([shape, [point_fore_head_x, point_fore_head_y]])\n",
    "                            # chin\n",
    "                            point_9_x = shape[8][0]\n",
    "                            point_9_y = shape[8][1]\n",
    "                            point_52_x = shape[51][0]\n",
    "                            point_52_y = shape[51][1]\n",
    "                            point_chin_x = np.int16(0.5*point_9_x+0.5*point_52_x)\n",
    "                            point_chin_y = np.int16(0.5*point_9_y+0.5*point_52_y)\n",
    "                            shape = np.vstack([shape, [point_chin_x, point_chin_y]])\n",
    "                            # throat\n",
    "                            # 臉向右的話,取左側,臉向左,取右側\n",
    "\n",
    "                            point_52_x = shape[51][0]\n",
    "                            point_52_y = shape[51][1]\n",
    "                            if shape[30][0]-shape[27][0] > 0:\n",
    "                                point_8_x = shape[7][0]\n",
    "                                point_8_y = shape[7][1]\n",
    "                                point_throat_x = np.int16(1.4*point_8_x-0.4*point_52_x)\n",
    "                                point_throat_y = np.int16(1.4*point_8_y-0.4*point_52_y)\n",
    "                            else:\n",
    "                                point_10_x = shape[9][0]\n",
    "                                point_10_y = shape[9][1]\n",
    "                                point_throat_x = np.int16(1.4*point_10_x-0.4*point_52_x)\n",
    "                                point_throat_y = np.int16(1.4*point_10_y-0.4*point_52_y)\n",
    "                            shape = np.vstack([shape, [point_throat_x, point_throat_y]])\n",
    "                            gray_sub_list = []\n",
    "\n",
    "                            n_frame = n_frame+1\n",
    "                            for i in range(self.n_feature_points):\n",
    "                                selected_point_y = shape[i][1]\n",
    "                                selected_point_x = shape[i][0]\n",
    "                                gray_color = frame[selected_point_y, selected_point_x]\n",
    "                                # 在adaptive_length之後才開始檢查數值是否在一個合理的範圍\n",
    "                                if n_frame > self.adaptive_length:\n",
    "                                    if (np.abs(gray_color-average_list[i])/average_list[i]) < 0.5:\n",
    "                                        gray_sub_list.append(gray_color)\n",
    "                                        average_list[i] = (average_list[i]*(n_frame-1)+gray_color)/n_frame  # 更新平均值\n",
    "                                    else:\n",
    "                                        if gray_color < self.minimum_gray_color:\n",
    "                                            # 小於最低值的,捨棄改用minimum_gray_color代替\n",
    "                                            gray_sub_list.append(average_list[i])\n",
    "                                            average_list[i] = average_list[i]  # 這句非必要只是容易看\n",
    "                                        else:\n",
    "                                            gray_sub_list.append(average_list[i])  # 變動超過範圍的,回傳平均值\n",
    "                                            average_list[i] = (average_list[i]*(n_frame-1)+gray_color)/n_frame  # 更新平均值\n",
    "                                else:\n",
    "                                    if gray_color < self.minimum_gray_color:\n",
    "                                        # 小於最低值的,捨棄改用minimum_gray_color代替\n",
    "                                        gray_sub_list.append(self.minimum_gray_color)\n",
    "                                        average_list[i] = average_list[i]  # 這句非必要只是容易看\n",
    "                                    else:\n",
    "                                        gray_sub_list.append(gray_color)\n",
    "                                        average_list[i] = (average_list[i]*(n_frame-1)+gray_color)/n_frame  # 更新平均值\n",
    "\n",
    "                            previous_gray_sub_list = gray_sub_list\n",
    "                            gray_list.append(gray_sub_list)\n",
    "                            gray_list_for_chart.append(gray_sub_list)\n",
    "\n",
    "                            # loop over the (x, y)-coordinates from our dlib shape\n",
    "                            # predictor model draw them on the image\n",
    "                            for (sx, sy) in shape:\n",
    "                                cv2.circle(frame_copy, (sx, sy), 2, (255, 0, 0), -1)\n",
    "\n",
    "                            # 加點\n",
    "                            cv2.circle(frame_copy, (point_left_cheek_x, point_left_cheek_y), 1, (0, 0, 255), -1)\n",
    "                            cv2.circle(frame_copy, (point_left_nose_x, point_left_nose_y), 1, (0, 0, 255), -1)\n",
    "                            cv2.circle(frame_copy, (point_right_cheek_x, point_right_cheek_y), 1, (0, 0, 255), -1)\n",
    "                            cv2.circle(frame_copy, (point_right_nose_x, point_right_nose_y), 1, (0, 0, 255), -1)\n",
    "                            cv2.circle(frame_copy, (point_fore_head_x, point_fore_head_y), 1, (0, 0, 255), -1)\n",
    "                            cv2.circle(frame_copy, (point_chin_x, point_chin_y), 1, (0, 0, 255), -1)\n",
    "                            cv2.circle(frame_copy, (point_throat_x, point_throat_y), 1, (0, 0, 255), -1)\n",
    "                        except Exception as inst:\n",
    "                            n_error = n_error + 1\n",
    "                            print(\"error:\")\n",
    "                            print(n_error)\n",
    "                            print(inst)\n",
    "                            gray_list_for_chart.append(previous_gray_sub_list)  # 補足frame以畫圖\n",
    "                    else:\n",
    "                        print(\"h out of boundary or y changes\")\n",
    "                        print(\"upper:\" + str(self.height_upper_threshold) + \";h=\" + str(h) + \";lower=\" + str(self.height_lower_threshold))\n",
    "                        print(\"y=\" + str(y) + \";y_average=\" + str(y_average))\n",
    "                        gray_list_for_chart.append(previous_gray_sub_list)  # 補足frame以畫圖\n",
    "            else:\n",
    "                print(\"len(rects)==0\")\n",
    "                testing_image_fullname = os.path.join(self.no_rectangle_image_path, str(n_grab) + \".png\")\n",
    "                cv2.imwrite(testing_image_fullname, frame)\n",
    "                gray_list_for_chart.append(previous_gray_sub_list)  # 補足frame以畫圖\n",
    "\n",
    "            # if the frame dimensions are empty, grab them\n",
    "            if W is None or H is None:\n",
    "                (H, W) = frame.shape[:2]\n",
    "\n",
    "            # check if the video writer is None\n",
    "            if writer is None:\n",
    "                # initialize our video writer\n",
    "                fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "                writer = cv2.VideoWriter(output_file_fullname, fourcc, 28, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "            # push the frame to the writer\n",
    "            writer.write(frame_copy)\n",
    "\n",
    "        if len(gray_list_for_chart) > 0:\n",
    "            print(file_name)\n",
    "            # print(gray_list_for_chart)\n",
    "            plt.rcParams[\"figure.figsize\"] = (15, 12)\n",
    "            plt.title(file_name)\n",
    "            plt.ylim([0, 255])\n",
    "            rolling_n = 10  # default=10\n",
    "            import pandas as pd\n",
    "            row_21 = pd.DataFrame([row[21] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_22 = pd.DataFrame([row[22] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_58 = pd.DataFrame([row[58] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_29 = pd.DataFrame([row[29] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_30 = pd.DataFrame([row[30] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_54 = pd.DataFrame([row[54] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_55 = pd.DataFrame([row[55] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_59 = pd.DataFrame([row[59] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "            row_60 = pd.DataFrame([row[60] for row in gray_list_for_chart]).rolling(rolling_n).mean()[rolling_n:]\n",
    "\n",
    "            plt.plot(row_21, label=\"Left Eyebrow\")\n",
    "            plt.plot(row_22, label=\"Right Eyebrow\")\n",
    "            plt.plot(row_58, label=\"Fore Head\")\n",
    "            plt.plot(row_29, label=\"Next To Nose Tip\")\n",
    "            plt.plot(row_30, label=\"Nose Tip\")\n",
    "            plt.plot(row_54, label=\"Left Cheek\")\n",
    "            plt.plot(row_55, label=\"Left Nose\")\n",
    "            plt.plot(row_59, label=\"Chin\")\n",
    "            plt.plot(row_60, label=\"Throat\")\n",
    "            # print(row_21.to_string(), row_22, row_58, row_29, row_30,  row_54, row_55, row_59, row_60)\n",
    "            # a = [row[21] for row in gray_list_for_chart]\n",
    "            # print(a)\n",
    "            # print(pd.DataFrame(a).rolling(10).mean()[10:])\n",
    "            # plt.legend(loc=\"upper left\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            # plt.title('test')\n",
    "            plt.tight_layout()\n",
    "            output_image_fullname = os.path.join(os.getcwd(), self.visualization_path, file_name + \".png\")\n",
    "            plt.savefig(output_image_fullname)\n",
    "            fig1 = plt.figure(file_name)\n",
    "            # plt.show()\n",
    "            print(os.path.join(os.getcwd(), output_csv_file_fullname))\n",
    "            np.savetxt(os.path.join(os.getcwd(), output_csv_file_fullname), gray_list, delimiter=\",\")\n",
    "\n",
    "            vs.release()\n",
    "            writer.release()\n",
    "            # do a bit cleanup\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            return np.array(gray_list), [f\"thermal_feature_{i}\" for i in range(61)]\n",
    "\n",
    "    def dataframe_to_list_of_lists(self, df):\n",
    "        \"\"\"\n",
    "        Converts a DataFrame into a list of lists and returns both the list and the column names.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame to convert.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - list of lists (each sublist is a row from the DataFrame)\n",
    "            - list of column names\n",
    "        \"\"\"\n",
    "        # Convert DataFrame to list of lists\n",
    "        list_of_lists = df.values.tolist()\n",
    "        \n",
    "        # Get column names as a list\n",
    "        column_names = df.columns.tolist()\n",
    "        \n",
    "        return np.array(list_of_lists), column_names\n",
    "    \n",
    "    def summarized_file_process(self,input_file_fullname):\n",
    "\n",
    "        import psychai.feature.feature_extraction.feature_processor\n",
    "\n",
    "        # Extract features using multiple strategies and plot them\n",
    "        strategies = ['mean', 'variance', 'max', 'end_to_begin']\n",
    "\n",
    "        features, feature_names = self.file_process(input_file_fullname)\n",
    "\n",
    "        # Summarize features using the specified strategies\n",
    "        processor = psychai.feature.feature_extraction.feature_processor.FeatureProcessor()\n",
    "        summarized_features, summarized_feature_names = processor.summarize_features(features, strategies, feature_names)\n",
    "        # Append the element to the array\n",
    "        # summarized_features = np.append(summarized_features, input_file_fullname)\n",
    "        # summarized_feature_names.append(\"files\") \n",
    "        return summarized_features, summarized_feature_names        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"../resources/thermal\"  # 实际上是视频\n",
    "model_path = \"../resources/models/thermal\"\n",
    "output_file_path = \"../results/thermal\"\n",
    "upsampling_times = 1\n",
    "n_feature_points = 54+7  # 臉部特徵點的數量\n",
    "n_sampling = 30  # downsampling\n",
    "adaptive_length = np.int16(30*10/n_sampling)  # 多少禎以後，限制數值變化的範圍\n",
    "resize_height = 600  # 調整影像的大小。600時，大概每秒可以處理1禎，但是如果下降到300，只需要1/100的時間。不過，識別正確性會減低。\n",
    "height_lower_threshold = np.int16(resize_height*0.2)\n",
    "height_upper_threshold = np.int16(resize_height*0.8)\n",
    "x_average = 0\n",
    "y_average = 0\n",
    "minimum_gray_color = 30  # 設一個最低的值,以避免抓到奇怪的數值\n",
    "\n",
    "para_dict = {\n",
    "    \"n_feature_points\": n_feature_points,\n",
    "    \"n_sampling\": n_sampling,\n",
    "    \"resize_height\": resize_height,\n",
    "    \"upsampling_times\": upsampling_times,\n",
    "    \"minimum_gray_color\": minimum_gray_color\n",
    "}\n",
    "\n",
    "feature_extractor = features_extraction(model_path, input_file_path, output_file_path, para_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psychai.feature.feature_extraction.feature_processor\n",
    "\n",
    "file = r'D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\raw_data\\005\\Segmented_Thermal\\PS-9_005_23_05_25_13_10_35.mp4'\n",
    "features, feature_names = feature_extractor.file_process(file)\n",
    "\n",
    "# Extract features using multiple strategies and plot them\n",
    "strategies = ['mean', 'variance', 'max', 'end_to_begin']\n",
    "processor = psychai.feature.feature_extraction.feature_processor.FeatureProcessor()\n",
    "\n",
    "summarized_features, summarized_feature_names = processor.summarize_features(features, strategies, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\raw_data\\005\\Segmented_Thermal\\PS-9_005_23_05_25_13_10_35.mp4'\n",
    "summarized_features, summarized_feature_names  = feature_extractor.summarized_file_process(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_directory = r\"D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\raw_data\"\n",
    "base_directory = r\"K:\\Backup\\Experiments\\Moral Elevation\\Disk1_2_combined\\FS-9 Day 1\"\n",
    "\n",
    "# Create a FileFilter instance with multiple attribute filters\n",
    "filters = [(2, '23')]\n",
    "file_filter = utilities.FileFilter(base_directory, attribute_filters=filters, modality_value='Segmented_Thermal')\n",
    "\n",
    "# Get matching files for user IDs from 1 to 13\n",
    "df = file_filter.get_matching_files(range(1, 150))\n",
    "\n",
    "# Load the rct.csv file\n",
    "rct_df = pd.read_csv('../resources/data/rct/rct.csv')\n",
    "\n",
    "# Merge the two dataframes based on the 'user_id' column\n",
    "df_updated = pd.merge(df, rct_df[['user_id', 'Group']], on='user_id', how='left')\n",
    "\n",
    "# Find the row with the largest file_size for each user_id\n",
    "df_largest = df_updated.loc[df_updated.groupby('user_id')['file_size'].idxmax()]\n",
    "\n",
    "# Reset the index if needed\n",
    "df_largest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(df_largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psychai.feature.feature_extraction.feature_retriever as retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate FeatureProcessor with the custom feature extractors\n",
    "processor = retriever.FeatureRetriever(df_largest, summarized_feature_extractors=[feature_extractor.summarized_file_process]\n",
    "                             ,cache_file_path=\"../results/thermal/thermal_features_cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and return the final DataFrame\n",
    "df_with_features = processor.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Set display options for full DataFrame output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "display(df_with_features)\n",
    "\n",
    "df_with_features.to_csv(r\"D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\results\\thermal\\summarized_results\\df_with_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "- Copyright：Ivan Liu \n",
    "- Last Update: 2024\n",
    "- Env：course_9 for feature extraction, psychai241104 for emotion recognition\n",
    "- Notes:\n",
    "    - install dlib by \"conda install -c conda-forge dlib\" reference :https://pyimagesearch.com/2018/01/22/install-dlib-easy-complete-guide/\n",
    "    - Usage: \\$ python dlib_predict_video.py --input video/2_0.avi --models  models/ --upsample 1 --output demo/output.mp4\n",
    "- References:\n",
    "    - https://github.com/IS2AI/thermal-facial-landmarks-detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psychai241104",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

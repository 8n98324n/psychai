{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import FileVideoStream\n",
    "from IPython.display import display, HTML\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psychai\n",
    "import psychai.feature.feature_extraction.feature_processor\n",
    "\n",
    "# Get the parent directory\n",
    "# This is useful if you need to import modules from a parent directory.\n",
    "import sys\n",
    "parent_dir = os.path.abspath('../')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import any custom utilities from the parent directory (if needed)\n",
    "import utilities\n",
    "\n",
    "path_pose_features_cache = r\"../results/pose/pose_features_cache_20241101_2.pkl\"\n",
    "path_pose_features_csv = r\"../results/pose/pose_features_cache_20241108.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = psychai.feature.feature_extraction.feature_processor.FeatureProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureProcessor:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def summarize_features(self, pose_features, strategies, all_feature_names):\n",
    "#         \"\"\"\n",
    "#         Summarize the extracted features based on the provided strategies (e.g., mean, variance).\n",
    "\n",
    "#         Args:\n",
    "#         pose_features (np.array): Extracted pose features for each frame.\n",
    "#         strategies (list): List of summarization strategies to apply.\n",
    "#         all_feature_names (list): Original feature names.\n",
    "\n",
    "#         Returns:\n",
    "#         np.array: Combined summarized features.\n",
    "#         list: Updated list of feature names with strategy prefixes.\n",
    "#         \"\"\"\n",
    "#         all_features = []\n",
    "#         combined_feature_names = []\n",
    "\n",
    "#         # Apply each summarization strategy (e.g., mean, variance) to the features\n",
    "#         for strategy in strategies:\n",
    "#             if strategy == 'mean':\n",
    "#                 summarized_features = self.summarize_mean(pose_features)\n",
    "#             elif strategy == 'variance':\n",
    "#                 summarized_features = self.summarize_variance(pose_features)\n",
    "#             elif strategy == 'end_to_begin':\n",
    "#                 summarized_features = self.summarize_end_to_begin(pose_features, len(pose_features))\n",
    "#             elif strategy == 'max':\n",
    "#                 summarized_features = self.summarize_max(pose_features)\n",
    "#             else:\n",
    "#                 raise ValueError(f\"Invalid strategy '{strategy}'\")\n",
    "\n",
    "#             feature_names = self.add_strategy_prefix(strategy, all_feature_names)\n",
    "#             all_features.append(summarized_features)\n",
    "#             combined_feature_names.extend(feature_names)\n",
    "\n",
    "#         return np.hstack(all_features), combined_feature_names\n",
    "    \n",
    "    \n",
    "#     # Helper functions for summarization and feature names (unchanged from original code)\n",
    "\n",
    "#     def summarize_mean(self, pose_features):\n",
    "#         return np.mean(pose_features, axis=0) if pose_features.size else np.array([])\n",
    "\n",
    "#     def summarize_variance(self, pose_features):\n",
    "#         return np.var(pose_features, axis=0) if pose_features.size else np.array([])\n",
    "\n",
    "#     def summarize_end_to_begin(self, pose_features, total_frames):\n",
    "#         tenth_frames = max(10, total_frames // 10)\n",
    "#         first_tenth = pose_features[:tenth_frames]\n",
    "#         last_tenth = pose_features[-tenth_frames:]\n",
    "#         first_mean = np.mean(first_tenth, axis=0)\n",
    "#         last_mean = np.mean(last_tenth, axis=0)\n",
    "#         return last_mean / (first_mean + 1e-6)\n",
    "\n",
    "#     def summarize_max(self, pose_features):\n",
    "#         return np.max(np.abs(pose_features), axis=0) if pose_features.size else np.array([])\n",
    "\n",
    "#     def add_strategy_prefix(self, strategy, feature_names):\n",
    "#         return [f\"{strategy}_{name}\" for name in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose and Drawing modules\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "\n",
    "class PoseFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to process pose data from a video and extract pose features using MediaPipe Pose.\n",
    "    \n",
    "    Attributes:\n",
    "    video_path (str): Path to the input video.\n",
    "    frame_rate (int): Frame rate of the video for sampling.\n",
    "    downsample_factor (int): How often to process a frame (e.g., process every nth frame).\n",
    "    custom_features (list): List of custom features to extract.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, video_path, frame_rate=30, downsample_factor=1, custom_features=None):\n",
    "        self.video_path = video_path  # Path to the video file\n",
    "        self.frame_rate = frame_rate  # Frame rate to control video sampling\n",
    "        self.downsample_factor = downsample_factor  # Frequency to downsample the video frames\n",
    "        self.custom_features = custom_features  # Any custom features that need to be calculated\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return [f'landmark_{i}_{axis}' for i in range(33) for axis in ['x', 'y', 'z']]\n",
    "\n",
    "    def process_frame(self, frame, frame_processor):\n",
    "        \"\"\"\n",
    "        Process a single frame to extract pose landmarks using MediaPipe.\n",
    "\n",
    "        Args:\n",
    "        frame (np.array): A single frame from the video.\n",
    "        frame_processor : object for processing.\n",
    "\n",
    "        Returns:\n",
    "        np.array: A list of flattened pose landmark coordinates (x, y, z) or None if no landmarks are found.\n",
    "        \"\"\"\n",
    "        # Convert the frame from BGR to RGB for MediaPipe processing\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # results = frame_processor.process(image_rgb)  # Process the frame to get pose landmarks\n",
    "        try:\n",
    "            results = frame_processor.process(image_rgb)  # Process the frame to get pose landmarks\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame: {e}\")\n",
    "            results = None  # Handle the error by setting results to None or any fallback value\n",
    "            \n",
    "        # If pose landmarks are detected, extract them\n",
    "        if results.pose_landmarks:\n",
    "            frame_landmarks = [landmark for lm in results.pose_landmarks.landmark\n",
    "                               for landmark in (lm.x, lm.y, lm.z)]  # Flatten landmarks into a list\n",
    "            return np.array(frame_landmarks)\n",
    "        return None  # Return None if no landmarks are found\n",
    "\n",
    "\n",
    "    def add_custom_features(self, frame_landmarks, all_feature_names):\n",
    "        \"\"\"\n",
    "        Add custom features to the frame landmarks if any custom features are defined.\n",
    "\n",
    "        Args:\n",
    "        frame_landmarks (np.array): The pose landmarks for a frame.\n",
    "        all_feature_names (list): List of feature names.\n",
    "\n",
    "        Returns:\n",
    "        np.array: Updated landmarks with custom features (if any).\n",
    "        \"\"\"\n",
    "        if self.custom_features:\n",
    "            custom_values, custom_names = self.calculate_custom_features_row(frame_landmarks, self.custom_features)\n",
    "            frame_landmarks = np.hstack([frame_landmarks, custom_values])\n",
    "            # Add custom feature names to the full feature list\n",
    "            if not hasattr(self, 'custom_names_extended'):\n",
    "                all_feature_names.extend(custom_names)\n",
    "                self.custom_names_extended = True\n",
    "        return frame_landmarks\n",
    "\n",
    "\n",
    "\n",
    "    def plot_features(self, pose_features, processed_frames, all_feature_names, features_to_plot):\n",
    "        \"\"\"\n",
    "        Plot selected features over time.\n",
    "\n",
    "        Args:\n",
    "        pose_features (np.array): Extracted pose features over time.\n",
    "        processed_frames (int): Number of processed frames.\n",
    "        all_feature_names (list): List of all feature names.\n",
    "        features_to_plot (list): List of features to plot (optional).\n",
    "        \"\"\"\n",
    "        if features_to_plot is None or len(features_to_plot) == 0:\n",
    "            features_to_plot = []\n",
    "        elif features_to_plot == ['All']:\n",
    "            features_to_plot = all_feature_names\n",
    "\n",
    "        feature_indices = [all_feature_names.index(f) for f in features_to_plot if f in all_feature_names]\n",
    "\n",
    "        if feature_indices:\n",
    "            time_stamps = np.arange(0, processed_frames) * (self.downsample_factor / self.frame_rate)\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            for idx in feature_indices:\n",
    "                plt.plot(time_stamps, pose_features[:, idx], label=all_feature_names[idx])\n",
    "\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Feature Value')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.title('Pose Features Over Time')\n",
    "            plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "            plt.show()\n",
    "\n",
    "    def calculate_custom_features_row(self, pose_row, custom_features):\n",
    "        custom_feature_values = []\n",
    "        custom_feature_names = []\n",
    "        for custom in custom_features:\n",
    "            operation = custom[0]\n",
    "            if operation in ['sum', 'difference']:\n",
    "                feature_a, feature_b, new_name = custom[1], custom[2], custom[3]\n",
    "                a_values = pose_row[feature_a * 3:feature_a * 3 + 3]\n",
    "                b_values = pose_row[feature_b * 3:feature_b * 3 + 3]\n",
    "                result_values = a_values + b_values if operation == 'sum' else a_values - b_values\n",
    "                custom_feature_values.extend(result_values)\n",
    "                custom_feature_names.extend([f\"{new_name}_{axis}\" for axis in ['x', 'y', 'z']])\n",
    "            elif operation == 'angle':\n",
    "                p0 = pose_row[custom[1] * 3: custom[1] * 3 + 3]\n",
    "                p1 = pose_row[custom[2] * 3: custom[2] * 3 + 3]\n",
    "                p2 = pose_row[custom[3] * 3: custom[3] * 3 + 3]\n",
    "                v1, v2 = p0 - p1, p2 - p1\n",
    "                angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "                custom_feature_values.append(angle)\n",
    "                custom_feature_names.append(custom[4])\n",
    "        return np.array(custom_feature_values), custom_feature_names\n",
    "\n",
    "\n",
    "\n",
    "    # This one works well:\n",
    "\n",
    "    def process_video(self, frame_processor):\n",
    "        \"\"\"\n",
    "        Process the video frame by frame to extract pose features and apply summarization strategies.\n",
    "\n",
    "        Args:\n",
    "        frame_processor (callable): A callable that processes a frame and returns pose landmarks.\n",
    "        use_parallel (bool): Whether to use parallel processing for faster computation.\n",
    "\n",
    "        Returns:\n",
    "        list: Names of the summarized features.\n",
    "        np.array: Summarized pose features.\n",
    "        int: Count of processed frames.\n",
    "        \"\"\"\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "\n",
    "        pose_features = []  # Store pose features of all processed frames\n",
    "        all_feature_names = self.get_feature_names()  # Get default pose feature names\n",
    "        processed_frames = 0  # Initialize processed frame counter\n",
    "\n",
    "        frame_index = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Downsample frames: Only process every nth frame\n",
    "            if frame_index % self.downsample_factor == 0:\n",
    "                frame_landmarks = self.process_frame(frame, frame_processor)\n",
    "                if frame_landmarks is not None:\n",
    "                    # Add custom features if needed\n",
    "                    frame_landmarks = self.add_custom_features(frame_landmarks, all_feature_names)\n",
    "                    pose_features.append(frame_landmarks)\n",
    "                    processed_frames += 1  # Increment only for successfully processed frames\n",
    "            frame_index += 1\n",
    "        cap.release()\n",
    "\n",
    "        # Convert the pose features list to a numpy array\n",
    "        pose_features = np.array(pose_features)\n",
    "\n",
    "        return all_feature_names, pose_features, processed_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarized_process_folder(file):\n",
    "    return extract_pose_features(file, False)\n",
    "\n",
    "def get_frame_rate(file_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    \n",
    "    # Retrieve the frame rate\n",
    "    frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Release the video file\n",
    "    video.release()\n",
    "    \n",
    "    # Return the frame rate\n",
    "    return frame_rate\n",
    "\n",
    "def extract_pose_features(file, show_graph):\n",
    "    custom_features = [\n",
    "        ['difference', 7, 8, 'ear_to_ear_difference'],\n",
    "        ['difference', 11, 12, 'shoulder_to_shoulder_difference'],\n",
    "        ['difference', 0, 9, 'nose_to_lip_difference'],\n",
    "        ['angle', 0, 12, 11, 'left_head_angle'],\n",
    "        ['angle', 12, 11, 0, 'right_head_angle']\n",
    "    ]\n",
    "    sampling_time = 30\n",
    "    frame_rate = get_frame_rate(file)\n",
    "    skip_frames = int(sampling_time * frame_rate)\n",
    "\n",
    "    extractor = PoseFeatureExtractor(file, frame_rate, downsample_factor=skip_frames, custom_features=custom_features)\n",
    "\n",
    "    # Extract features using multiple strategies and plot them\n",
    "    strategies = ['mean', 'variance', 'max', 'end_to_begin']\n",
    "\n",
    "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        feature_names, features, processed_frames = extractor.process_video(pose)\n",
    "\n",
    "    # processor = FeatureProcessor()\n",
    "    # Summarize features using the specified strategies\n",
    "    processor = psychai.feature.feature_extraction.feature_processor.FeatureProcessor()\n",
    "\n",
    "    # Summarize features using the specified strategies\n",
    "    summarized_features, summarized_feature_names = processor.summarize_features(features, strategies, feature_names)\n",
    "\n",
    "    if show_graph:\n",
    "        # Plot selected features over time, if requested\n",
    "        extractor.plot_features(features, processed_frames, feature_names, features_to_plot=\n",
    "                                                        ['ear_to_ear_difference_x','ear_to_ear_difference_y',\n",
    "                                                        'shoulder_to_shoulder_difference_x','shoulder_to_shoulder_difference_y',\n",
    "                                                        'nose_to_lip_difference_x','nose_to_lip_difference_y',\n",
    "                                                        'left_head_angle','right_head_angle'])\n",
    "\n",
    "        print(processed_frames)\n",
    "        print(len(feature_names))\n",
    "        print(len(features))\n",
    "        print(features[0])\n",
    "\n",
    "    return summarized_features, summarized_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe image example\n",
    "\n",
    "这段代码通过MediaPipe的Pose模型来检测图像中人体的姿态，包括关键点的定位和身体部位之间的连接。代码首先配置了Pose模型，包括开启静态图像模式、设置模型复杂度、启用背景分割以及设置最小检测置信度。接着，读取一张图像，将其从BGR格式转换为RGB格式，并通过Pose模型处理。处理结果包括身体姿态的关键点定位和背景分割掩码。代码接着创建了一个注释图像，其中背景被设置为灰色，并在图像上绘制了身体的姿态关键点和连接线。最后，显示了注释图像，并绘制了Pose世界坐标系中的关键点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_1 = r\"D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\images\\pose_example_1.jpg\"\n",
    "image_path_2 = r\"D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\images\\pose_example_2.jpg\"\n",
    "\n",
    "\n",
    "def mediapipe_draw(image):\n",
    "    # 初始化绘图工具和绘图风格\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    # 初始化Pose检测模型\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # 设置背景颜色为灰色\n",
    "    BG_COLOR = (192, 192, 192)  # 灰色\n",
    "\n",
    "    # 配置Pose模型参数\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=True,  # 静态图像模式\n",
    "        model_complexity=2,  # 模型复杂度\n",
    "        enable_segmentation=True,  # 启用背景分割\n",
    "        min_detection_confidence=0.5)  # 最小检测置信度\n",
    "\n",
    "\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    # 在处理前将BGR图像转换为RGB\n",
    "    converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(converted_image)\n",
    "\n",
    "    # 复制原图像用于注释\n",
    "    annotated_image = converted_image.copy()\n",
    "\n",
    "    # 在图像上绘制分割结果\n",
    "    # 如果需要在边界周围改进分割效果，可以考虑对\"results.segmentation_mask\"和\"image\"应用联合双边滤波\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "    bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "    bg_image[:] = BG_COLOR  # 设置背景图像为灰色\n",
    "    annotated_image = np.where(condition, annotated_image, bg_image)  # 应用条件，合成最终的注释图像\n",
    "\n",
    "    # 定义更粗的线条和关键点样式\n",
    "    thicker_line_spec = mp_drawing.DrawingSpec(thickness=6, circle_radius=3, color=(0,0,255))\n",
    "    thicker_connection_spec = mp_drawing.DrawingSpec(thickness=4, color=(0,255,0))\n",
    "\n",
    "\n",
    "    # 在图像上绘制Pose特征点和连接线\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=thicker_line_spec,  # 使用自定义的关键点样式\n",
    "        connection_drawing_spec=thicker_connection_spec  # 使用自定义的连接线样式\n",
    "    )\n",
    "\n",
    "    # 显示带有Pose特征点的图像\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制Pose的世界坐标系中的特征点\n",
    "    mp_drawing.plot_landmarks(\n",
    "        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取图像\n",
    "image_1 = cv2.imread(image_path_1)\n",
    "mediapipe_draw(image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取图像\n",
    "image_2 = cv2.imread(image_path_2)\n",
    "mediapipe_draw(image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openpose single image example\n",
    "\n",
    "OpenPose is an open-source library for real-time multi-person 2D pose estimation, capable of detecting key points for the body, face, hands, and feet. Originally developed by the Perceptual Computing Lab at Carnegie Mellon University, it has become a benchmark project in the field of pose estimation.\n",
    "\n",
    "Key Features\n",
    "- Real-Time Capability: OpenPose can detect poses in real-time video streams, making it suitable for applications requiring immediate feedback, such as interactive art, sports analysis, and health monitoring.\n",
    "- Multi-Person Detection: Unlike some pose estimation systems that can only handle a single person, OpenPose can detect poses for multiple people simultaneously in complex scenes.\n",
    "- Multi-Part Detection: It can detect and track key points across multiple body parts, including the body, face, hands, and feet.\n",
    "- Cross-Platform: OpenPose supports Windows, Linux, and Mac operating systems and can run on various hardware setups, including NVIDIA GPUs and CPUs.\n",
    "- Open Source: As an open-source project, OpenPose encourages community contributions and application development, which continuously enhances its capabilities and broadens its applicability.\n",
    "\n",
    "Here, we use features extracted with OpenPose online and a DNN model (graph_opt.pb) trained on the MobileNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "\n",
    "image_width=368\n",
    "image_height=368\n",
    "# 门槛值\n",
    "threshold=0.2\n",
    "# 將圖片讀入\n",
    "img_1 = cv.imread(image_path_1,cv.IMREAD_UNCHANGED)\n",
    "img_2 = cv.imread(image_path_2,cv.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appen_detection_result_2(raw_image, image_file):\n",
    "\n",
    "    # 載入預訓練好的模型\n",
    "    net = cv.dnn.readNetFromTensorflow(\"D:/Programming3/psychai/tutorial_simplified_chinese/feature_extraction/models/openpose/graph_opt.pb\")\n",
    "    \n",
    "    photo_height=image_file.shape[0]\n",
    "    photo_width=image_file.shape[1]\n",
    "    net.setInput(cv.dnn.blobFromImage(image_file, 1.0, (image_width, image_height), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :] \n",
    "\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "            # Slice heatmap of corresponging body's part.\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "            # Originally, we try to find all the local maximums. To simplify a sample\n",
    "            # we just find a global one. However only a single pose at the same time\n",
    "            # could be detected this way.\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        x = (photo_width * point[0]) / out.shape[3]\n",
    "        y = (photo_height * point[1]) / out.shape[2]\n",
    "        # Add a point if it's confidence is higher than threshold.\n",
    "        points.append((int(x), int(y)) if conf > threshold else None)\n",
    "\n",
    "\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(raw_image, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(raw_image, points[idFrom], (6, 6), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(raw_image, points[idTo], (6, 6), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "\n",
    "    face_img = raw_image.copy() #建立复本,避免影响原本的图像\n",
    "    face_img_RGB = cv.cvtColor(face_img, cv.COLOR_BGR2RGB) # OpenCV用BGR顺序,而matplotlib用RGB, 所以必须做一次转换 \n",
    "    plt.imshow(face_img_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appen_detection_result_2(img_1, img_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appen_detection_result_2(img_2, img_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../resources/images/mediapipe_pose_landmarker.png'\n",
    "display(HTML(f'<img src=\"{image_path}\" style=\"height: 400px;\" />'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_analysis = utilities.SingleModalityAnalysis()\n",
    "\n",
    "# base_directory = r\"D:\\Programming3\\psychai\\example\\paper5_mmer_moral\\resources\\raw_data\"\n",
    "# base_directory = r\"K:\\Backup\\Experiments\\Moral Elevation\\Disk1_2_combined\\FS-9 Day 1\"\n",
    "base_directory = r\"F:\\Experiments\\Moral Elevation\\Disk1_2_combined\\FS-9 Day 1\"\n",
    "\n",
    "filters = [(2, '23')]\n",
    "user_ids = range(1,120)\n",
    "merging_pdf = pd.read_csv('../resources/data/rct/rct.csv')\n",
    "\n",
    "df_largest = single_analysis.prosss_folder(base_directory, filters, user_ids, merging_pdf, modality_value=\"RecordedVideo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psychai.feature.feature_extraction.feature_retriever as retriever\n",
    "\n",
    "processor = retriever.FeatureRetriever(df_largest, summarized_feature_extractors=[summarized_process_folder]\n",
    "                             ,cache_file_path=path_pose_features_cache)\n",
    "\n",
    "# Extract features and return the final DataFrame\n",
    "df_with_features = processor.extract_features()\n",
    "df_with_features.to_csv(path_pose_features_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "- Copyright：Ivan Liu \n",
    "- Last Update: 2024\n",
    "- Env：course_６ for feature extraction, psychai241104 for emotion recognition\n",
    "- References:\n",
    "    - https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
    "    - https://github.com/quanhua92/human-pose-estimation-opencv\n",
    "    - https://github.com/opencv/opencv/blob/master/samples/dnn/openpose.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psychai241104",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

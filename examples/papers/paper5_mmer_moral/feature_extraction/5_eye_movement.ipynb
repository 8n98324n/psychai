{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye-Movement Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psychai.feature.feature_extraction.feature_retriever as feature_retriever\n",
    "import psychai.feature.feature_extraction.feature_processor as feature_processor\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class EyeFeatureExtractor:\n",
    "    def __init__(self, output_video_folder_path= r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\paper5_mmer_moral\\results\\eye_movement\\videos\",\n",
    "                 result_cache_folder_path = r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\paper5_mmer_moral\\results\\eye_movement\\cache\", \n",
    "                 generate_output_video = False,):\n",
    "        self.output_video_folder_path = output_video_folder_path\n",
    "        self.generate_output_video = generate_output_video\n",
    "        self.result_cache_folder_path = result_cache_folder_path\n",
    "        pass\n",
    "\n",
    "    def process_video(self, input_video_path):\n",
    "\n",
    "        # Initialize MediaPipe FaceMesh\n",
    "        mp_face_mesh = mp.solutions.face_mesh\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        base_name = os.path.basename(input_video_path)\n",
    "        out_put_file_path = os.path.join(self.output_video_folder_path,base_name)\n",
    "        #cache_file_path = os.path.join(self.result_cache_folder_path,base_name)\n",
    "        cache_file_path = os.path.join(self.result_cache_folder_path, f\"{os.path.splitext(base_name)[0]}.pkl\")\n",
    "\n",
    "        if os.path.exists(cache_file_path):\n",
    "            # Load df_results from the pickle file\n",
    "            with open(cache_file_path, 'rb') as file:\n",
    "                df_results = pickle.load(file)\n",
    "            print(f\"[File {input_video_path}] DataFrame loaded from pickle file.\")\n",
    "            return np.array(df_results), df_results.columns.tolist()\n",
    "\n",
    "        # Define landmark indices for the iris and eye corners and top/bottom of eye\n",
    "        LEFT_IRIS_CENTER = 468\n",
    "        RIGHT_IRIS_CENTER = 473\n",
    "        LEFT_EYE_OUTER_CORNER = 33\n",
    "        LEFT_EYE_INNER_CORNER = 133\n",
    "        RIGHT_EYE_OUTER_CORNER = 263\n",
    "        RIGHT_EYE_INNER_CORNER = 362\n",
    "        LEFT_EYE_TOP = 159\n",
    "        LEFT_EYE_BOTTOM = 145\n",
    "        RIGHT_EYE_TOP = 386\n",
    "        RIGHT_EYE_BOTTOM = 374\n",
    "\n",
    "\n",
    "        # Open the input video\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Unable to open video file {input_video_path}\")\n",
    "            exit()\n",
    "\n",
    "        # Get video properties for setting up the VideoWriter\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        if self.generate_output_video:\n",
    "            out = cv2.VideoWriter(out_put_file_path, fourcc, fps, (width, height))\n",
    "\n",
    "        # Initialize DataFrame to store results\n",
    "        results = []\n",
    "\n",
    "        # Initialize the FaceMesh model\n",
    "        with mp_face_mesh.FaceMesh(\n",
    "                static_image_mode=False,\n",
    "                max_num_faces=1,\n",
    "                refine_landmarks=True,  # This enables iris landmarks\n",
    "                min_detection_confidence=0.5,\n",
    "                min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "            frame_number = 0\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Convert the frame color to RGB\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Process the frame to detect face landmarks\n",
    "                results_landmarks = face_mesh.process(image_rgb)\n",
    "\n",
    "                if results_landmarks.multi_face_landmarks:\n",
    "                    for face_landmarks in results_landmarks.multi_face_landmarks:\n",
    "                        # Get the eye corners, iris centers, and top/bottom eye landmarks\n",
    "                        left_eye_outer = face_landmarks.landmark[LEFT_EYE_OUTER_CORNER]\n",
    "                        left_eye_inner = face_landmarks.landmark[LEFT_EYE_INNER_CORNER]\n",
    "                        left_eye_top = face_landmarks.landmark[LEFT_EYE_TOP]\n",
    "                        left_eye_bottom = face_landmarks.landmark[LEFT_EYE_BOTTOM]\n",
    "                        right_eye_outer = face_landmarks.landmark[RIGHT_EYE_OUTER_CORNER]\n",
    "                        right_eye_inner = face_landmarks.landmark[RIGHT_EYE_INNER_CORNER]\n",
    "                        right_eye_top = face_landmarks.landmark[RIGHT_EYE_TOP]\n",
    "                        right_eye_bottom = face_landmarks.landmark[RIGHT_EYE_BOTTOM]\n",
    "                        left_iris_center = face_landmarks.landmark[LEFT_IRIS_CENTER]\n",
    "                        right_iris_center = face_landmarks.landmark[RIGHT_IRIS_CENTER]\n",
    "\n",
    "                        # Calculate eye region width and height for left eye\n",
    "                        left_eye_width = abs(left_eye_outer.x - left_eye_inner.x)\n",
    "                        left_eye_height = abs(left_eye_top.y - left_eye_bottom.y)\n",
    "\n",
    "                        # Calculate relative position of left iris to the eye frame\n",
    "                        mean_left_y = (left_eye_outer.y + left_eye_inner.y) / 2\n",
    "                        left_iris_relative_x = (left_iris_center.x - left_eye_outer.x) / left_eye_width\n",
    "                        left_iris_relative_y = (mean_left_y - left_iris_center.y) / left_eye_width\n",
    "\n",
    "                        # Calculate eye region width and height for right eye\n",
    "                        right_eye_width = abs(right_eye_outer.x - right_eye_inner.x)\n",
    "                        right_eye_height = abs(right_eye_top.y - right_eye_bottom.y)\n",
    "\n",
    "                        # Calculate relative position of right iris to the eye frame\n",
    "                        mean_right_y = (right_eye_outer.y + right_eye_inner.y) / 2\n",
    "                        right_iris_relative_x = (right_iris_center.x - right_eye_inner.x) / right_eye_width\n",
    "                        right_iris_relative_y = (mean_right_y - right_iris_center.y) / right_eye_width\n",
    "\n",
    "                        # Append data to results list\n",
    "                        results.append({\n",
    "                            'file_path': input_video_path,\n",
    "                            'frame_number': frame_number,\n",
    "                            'left_iris_relative_x': left_iris_relative_x,\n",
    "                            'left_iris_relative_y': left_iris_relative_y,\n",
    "                            'right_iris_relative_x': right_iris_relative_x,\n",
    "                            'right_iris_relative_y': right_iris_relative_y,\n",
    "                            'left_eye_width': left_eye_width,\n",
    "                            'left_eye_height': left_eye_height,\n",
    "                            'right_eye_width': right_eye_width,\n",
    "                            'right_eye_height': right_eye_height\n",
    "                        })\n",
    "\n",
    "                        if self.generate_output_video:\n",
    "                            # Overlay the values on the video frame\n",
    "                            cv2.putText(frame, f'Frame: {frame_number}', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Left Iris X: {left_iris_relative_x:.3f}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Left Iris Y: {left_iris_relative_y:.3f}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Right Iris X: {right_iris_relative_x:.3f}', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Right Iris Y: {right_iris_relative_y:.3f}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Left Eye W: {left_eye_width:.3f}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Left Eye H: {left_eye_height:.3f}', (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Right Eye W: {right_eye_width:.3f}', (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                            cv2.putText(frame, f'Right Eye H: {right_eye_height:.3f}', (10, 180), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                            # Draw the face mesh with iris and eye points\n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                image=frame,\n",
    "                                landmark_list=face_landmarks,\n",
    "                                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                landmark_drawing_spec=None,\n",
    "                                connection_drawing_spec=mp_drawing.DrawingSpec(color=(216, 216, 216), thickness=1, circle_radius=1)\n",
    "                            )\n",
    "\n",
    "                # Write the processed frame to the output video\n",
    "                if self.generate_output_video:\n",
    "                    out.write(frame)\n",
    "                frame_number += 1\n",
    "\n",
    "            # Convert results to DataFrame\n",
    "            df_results = pd.DataFrame(results)\n",
    "\n",
    "            # Release resources\n",
    "            cap.release()\n",
    "            if self.generate_output_video:\n",
    "                out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "        with open(cache_file_path, 'wb') as file:\n",
    "            pickle.dump(df_results, file)\n",
    "            print(f\"[File {input_video_path}] DataFrame generated and saved to pickle file.\")\n",
    "\n",
    "        # Return the DataFrame with results\n",
    "        return np.array(df_results), df_results.columns.tolist()\n",
    "\n",
    "    def summarized_process_folder(self,input_video_path):\n",
    "         \n",
    "        # Extract features using multiple strategies and plot them\n",
    "        strategies = ['mean', 'variance', 'max', 'end_to_begin']\n",
    "        \n",
    "        try:\n",
    "            features, feature_names = self.process_video(input_video_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None, None   \n",
    "\n",
    "        # Summarize features using the specified strategies\n",
    "        processor = feature_processor.FeatureProcessor()\n",
    "        summarized_features, summarized_feature_names = processor.summarize_features(features, strategies, feature_names)\n",
    "\n",
    "        return summarized_features, summarized_feature_names   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\resources\\videos\\Ivan_Eye.mp4\"\n",
    "output_video_path = r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\paper5_mmer_moral\\results\\eye_movement\\videos\\PS-9_005_6_05_25_12_26_03.mp4\"\n",
    "extractor = EyeFeatureExtractor()\n",
    "features, column_names = extractor.summarized_process_folder(input_video_path,output_video_path )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame([features], columns=column_names)\n",
    "df_result = df_result.loc[:, ~df_result.columns.str.contains('frame')]\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "parent_dir = os.path.abspath('../')\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import any custom utilities from the parent directory (if needed)\n",
    "import utilities\n",
    "\n",
    "base_directory = r\"W:\"\n",
    "\n",
    "# Create a FileFilter instance with multiple attribute filters\n",
    "filters = [(2, '25'),(2, '26'),(2, '27'),(2, '28')]\n",
    "file_filter = utilities.FileFilter(base_directory, attribute_filters=filters, modality_value='RecordedVideo')\n",
    "\n",
    "# Get matching files for user IDs from 1 to 13\n",
    "df = file_filter.get_matching_files(range(1, 120))\n",
    "\n",
    "# Load the rct.csv file\n",
    "rct_df = pd.read_csv('../resources/data/rct/rct.csv')\n",
    "\n",
    "# Merge the two dataframes based on the 'user_id' column\n",
    "df_updated = pd.merge(df, rct_df[['user_id', 'Group']], on='user_id', how='left')\n",
    "\n",
    "# Find the row with the largest file_size for each user_id\n",
    "#df_largest = df_updated.loc[df_updated.groupby('user_id','attribute_value')['file_size'].idxmax()]\n",
    "# Group by 'user_id' and 'attribute_value', then select the rows with the largest 'file_size'\n",
    "df_largest = df_updated.loc[df_updated.groupby(['user_id', 'attribute_value'])['file_size'].idxmax()]\n",
    "\n",
    "\n",
    "# Reset the index if needed\n",
    "df_largest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(df_largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# parent_dir = os.path.abspath('../')\n",
    "# sys.path.append(parent_dir)\n",
    "\n",
    "# # Import any custom utilities from the parent directory (if needed)\n",
    "# import utilities\n",
    "\n",
    "# single_analysis = utilities.SingleModalityAnalysis()\n",
    "\n",
    "# #base_directory = r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\paper5_mmer_moral\\resources\\raw_data\"\n",
    "# base_directory = r\"W:\"\n",
    "\n",
    "# filters = [(2, '25'), (2, '26'), (2, '27'), (2, '28')]\n",
    "# user_ids = range(1, 120)\n",
    "# merging_pdf = pd.read_csv('../resources/data/rct/rct.csv')\n",
    "\n",
    "# df_largest = single_analysis.prosss_folder(base_directory, filters, user_ids, merging_pdf, modality_value=\"RecordedVideo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = EyeFeatureExtractor()\n",
    "processor = feature_retriever.FeatureRetriever(df_largest, summarized_feature_extractors=[extractor.summarized_process_folder]\n",
    "                             ,cache_file_path=\"../results/eye_movement/cache/eye_movement_features_cache_20241106.pkl\")\n",
    "\n",
    "df_with_features = processor.extract_features()\n",
    "df_with_features.to_csv(\"../results/eye_movement/csv/eye_movement_features_cache_20241106.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "- Copyright：Ivan Liu \n",
    "- Last Update: 2024\n",
    "- Env：psychai241104\n",
    "- References: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env41124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import Image as Image_Show\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "huggingface_cache_location = os.path.abspath(\"../../../\" + os.getenv(\"huggingface_cache_location\"))\n",
    "output_csv_path = \"../results/mmllm/csv_output/results.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    memory_usage = result.stdout.decode(\"utf-8\").strip().split('\\n')\n",
    "    for i, memory in enumerate(memory_usage):\n",
    "        used, total = memory.split(',')\n",
    "        print(f\"GPU {i}: {used.strip()} MiB / {total.strip()} MiB used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Uploads/Programming/resources/huggingface_cache'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_cache_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "                                          cache_dir=huggingface_cache_location, \n",
    "                                          local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752e9bdecd724bb9a40328c9f74a7325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    " \n",
    "model_quantized = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=huggingface_cache_location,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"\"\"\n",
    "            Use the following cues to describe a person in an image who may appear compassionate, inspired, both, or neither.\n",
    "\n",
    "            Keep the answer in 200 words.\n",
    "         \n",
    "            [Facial Features]\n",
    "\n",
    "            Soft Eyes: Describe if the person’s eyes seem warm and slightly squinted, with relaxed muscles around them, conveying empathy and warmth. Look for a gentle and welcoming expression.\n",
    "            Gentle Smile: Note if their smile is soft and genuine, rather than a broad grin, and whether it reaches their eyes, creating crow’s feet, indicating sincerity and warmth.\n",
    "            Raised Eyebrows: Check if their eyebrows are slightly raised, which can reflect openness and curiosity, suggesting inspiration or attentiveness.\n",
    "            Relaxed Mouth: Observe if their mouth is slightly open, perhaps indicating awe, or gently pressed, conveying care or understanding.\n",
    "            Head Tilt: Note any slight tilt of their head, a cue often associated with attentiveness, warmth, and openness to listening.\n",
    "            [Pose Features]\n",
    "\n",
    "            Open Body Language: Describe if their arms are uncrossed and relaxed, perhaps hanging comfortably at their sides or gently clasped, signaling approachability and openness.\n",
    "            Forward Lean: Notice if they lean slightly forward, which often signifies engagement, empathy, or eagerness to connect.\n",
    "            Gentle Gestures: If gestures are visible, assess if they are smooth and slow, possibly with open palms—a welcoming, trustworthy signal.\n",
    "            Hand on Heart or Chest: Look for any gesture where they place a hand on their chest, indicating sincerity and a personal connection to their emotions.\n",
    "            Relaxed Stance: Describe their posture. A relaxed yet alert stance, with slightly bent knees or relaxed shoulders, can suggest comfort, openness, and emotional receptiveness.\n",
    "         \"\"\"\n",
    "         }\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Exampe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../results/mmllm/frame_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "def print_time(description, video_file_base_name,  start):\n",
    "\n",
    "    # End the timer\n",
    "    end = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end - start\n",
    "\n",
    "    # Convert to seconds, minutes, and hours\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    minutes = int((elapsed_time // 60) % 60)\n",
    "    hours = int(elapsed_time // 3600)\n",
    "\n",
    "    # Print the result in a readable format\n",
    "    print(f\"[File:{video_file_base_name}] {description}. Elapsed time: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Load existing results if they exist\n",
    "def load_existing_results(output_csv_path):\n",
    "    if os.path.exists(output_csv_path):\n",
    "        return pd.read_csv(output_csv_path, sep='\\t')\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"file_path\", \"sequence\", \"decoded_text\"])\n",
    "    \n",
    "\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(image_path, input_text, processor, model_quantized, existing_results,max_new_tokens=100, override=False):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        # Extract file path and sequence number\n",
    "        base_name = os.path.basename(image_path)\n",
    "        file_path = base_name.split(\"_frame_\")[0] + \".mp4\"\n",
    "        sequence = int(base_name.split(\"_frame_\")[-1].split(\".\")[0])\n",
    "\n",
    "        # Check if result already exists\n",
    "        if not override:\n",
    "            if not existing_results.empty:\n",
    "                result = existing_results[(existing_results['file_path'] == file_path) & (existing_results['sequence'] == sequence)]\n",
    "                if not result.empty:\n",
    "                    # Return existing decoded_text without processing\n",
    "                    return result.iloc[0].to_dict()\n",
    "        \n",
    "        # Process the image if decoded_text doesn't exist\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model_quantized.device)\n",
    "        \n",
    "        # Generate model output\n",
    "        output = model_quantized.generate(**inputs, max_new_tokens=200)\n",
    "        decoded_text = processor.decode(output[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "        print_time(\"Input Decoded.\", base_name , start)\n",
    "        print(f\"file:{file_path}\\nmessage:{decoded_text}\")\n",
    "        # Return the new result\n",
    "        return {\"file_path\": file_path, \"sequence\": sequence, \"decoded_text\": decoded_text}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to process all files one by one\n",
    "def process_files_sequentially(folder_path, input_text, processor, model_quantized, output_csv_path,max_new_tokens, override):\n",
    "    # Load existing results\n",
    "    existing_results = load_existing_results(output_csv_path)\n",
    "    results = []\n",
    "\n",
    "    # List all image files in the folder\n",
    "    image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "    # Process each file one by one and track progress with tqdm\n",
    "    for image_path in tqdm(image_files, desc=\"Processing files\"):\n",
    "        result = process_file(image_path, input_text, processor, model_quantized, existing_results, max_new_tokens, override)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "    # Create a DataFrame from results\n",
    "    if results:\n",
    "        df_new_results = pd.DataFrame(results)\n",
    "        # Combine new results with existing results\n",
    "        df_combined = pd.concat([existing_results, df_new_results]).drop_duplicates(subset=[\"file_path\", \"sequence\"]).reset_index(drop=True)\n",
    "        # Save the combined results to CSV\n",
    "        df_combined.to_csv(output_csv_path, sep='\\t', index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "# Define input_text, processor, and model_quantized\n",
    "# input_text = \"Your input text here\"\n",
    "# processor = YourProcessorClass()  # Replace with actual processor instance\n",
    "# model_quantized = YourModelClass()  # Replace with actual model instance\n",
    "\n",
    "# process_files_sequentially(folder_path, input_text, processor, model_quantized, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[File:PS-9_001_23_05_21_12_42_54_frame_1.jpg] Input Decoded.. Elapsed time: 0 hours, 0 minutes, 17 seconds\n",
      "file:PS-9_001_23_05_21_12_42_54.mp4\n",
      "message:The person in the image appears to be a young man with black hair, wearing glasses and a black shirt. He is seated in front of a white wall, with a window to his left and a curtain pulled back. The overall atmosphere suggests a calm and relaxed setting.\n",
      "\n",
      "Upon closer inspection, the person's facial features convey a sense of compassion and inspiration. His soft eyes seem warm and slightly squinted, with relaxed muscles around them, conveying empathy and warmth. A gentle smile plays on his lips, reaching his eyes and creating crow's feet, indicating sincerity and warmth. His eyebrows are slightly raised, suggesting openness and curiosity, while his mouth is slightly open, possibly indicating awe or care. A subtle head tilt adds to the impression of attentiveness and warmth.\n",
      "\n",
      "In terms of pose features, the person's open body language is evident, with his arms uncrossed and relaxed, perhaps hanging comfortably at his sides or gently clasped. A slight forward lean suggests engagement and empathy, while gentle gestures\n",
      "Results saved to ../results/mmllm/csv_output/results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_files_sequentially(folder_path, input_text, processor, model_quantized, output_csv_path, 400, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Exampe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\paper5_mmer_moral\\results\\mmllm\\frame_output\\PS-9_001_23_05_21_12_42_54_frame_1.jpg\"\n",
    "Image_Show(image_path, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = r\"C:\\Working\\Programming\\Python\\psychai_v1\\example\\paper5_mmer_moral\\results\\mmllm\\csv_output\"\n",
    "\n",
    "result = process_file(image_path, input_text, processor, model_quantized, output_csv_path, pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'PS-9_001_23_05_21_12_42_54.mp4',\n",
       " 'sequence': 1,\n",
       " 'decoded_text': 'The person in the image appears to be experiencing compassion or inspiration. Their facial features convey a sense of warmth and empathy, with soft eyes that seem to welcome and understand. A gentle smile plays on their lips, reaching their eyes and creating a sense of sincerity. Their eyebrows are slightly raised, indicating openness and curiosity, while their mouth is relaxed, possibly'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "- Copyright：Ivan Liu \n",
    "- Last Update: 2024\n",
    "- Env：psychai241104\n",
    "- References: \n",
    "    - https://github.com/PrudhviGNV/Speech-Emotion-Recognization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psychai241104",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# ğŸ“Œ æ ‡å‡†åº“ï¼ˆPython å†…ç½®åº“ï¼‰
import os      # å¤„ç†æ–‡ä»¶è·¯å¾„ã€ç¯å¢ƒå˜é‡ç­‰
import re      # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºæ–‡æœ¬å¤„ç†
import json    # å¤„ç† JSON æ•°æ®ï¼ˆåºåˆ—åŒ–/ååºåˆ—åŒ–ï¼‰
import time    # å¤„ç†æ—¶é—´ï¼ˆå¦‚å»¶è¿Ÿã€æ—¶é—´æˆ³ï¼‰

# ğŸ“Œ æœºå™¨å­¦ä¹  & æ·±åº¦å­¦ä¹ åº“
import torch  # PyTorchï¼Œæ”¯æŒ GPU/CPU è®¡ç®—

# ğŸ“Œ äº‘ç«¯ API SDK
import boto3   # AWS SDKï¼Œç”¨äºè°ƒç”¨ AWS Bedrock API
import openai  # OpenAI SDKï¼Œç”¨äºè°ƒç”¨ OpenAI API
import pandas as pd  # Pandas åº“ï¼Œç”¨äºå¤„ç†æ•°æ®è¡¨æ ¼ï¼ˆDataFrameï¼‰

# ğŸ“Œ AI ä¾›åº”å•† & æ¨¡å‹ç®¡ç†
from enum import Enum  # æšä¸¾ç±»å‹ï¼ˆEnumï¼‰ï¼Œç”¨äºå­˜å‚¨ AI ä¾›åº”å•†å’Œæ¨¡å‹ä¿¡æ¯

# ğŸ“Œ Hugging Face `transformers` åº“ï¼ˆæœ¬åœ° & Hugging Face æ¨¡å‹ï¼‰
from transformers import AutoModelForCausalLM, AutoTokenizer  # NLP é¢„è®­ç»ƒæ¨¡å‹

# ğŸ“Œ çŸ¥ä¹ AI SDK
from zhipuai import ZhipuAI  # ç”¨äºè°ƒç”¨ çŸ¥ä¹ AI API

# ğŸ“Œ å­—èŠ‚è·³åŠ¨æ–¹èˆŸï¼ˆVolcEngine Arkï¼‰SDK
# éœ€è¦å…ˆå®‰è£…ï¼š`pip install volcengine-python-sdk[ark]`
from volcenginesdkarkruntime import Ark  # å¤„ç†æ–¹èˆŸ APIï¼ˆå­—èŠ‚è·³åŠ¨ï¼‰

# class model_helper:
#     def __init__(self):
#         pass


# ğŸ“Œ å®šä¹‰ AI ä¾›åº”å•†ï¼ˆProviderï¼‰
class LLMSource(Enum):
    """AI ä¾›åº”å•†æšä¸¾"""
    OPENAI = "openai"  # OpenAI API
    DEEPSEEK = "deepseek"  # DeepSeek API
    QWEN = "qwen"  # é˜¿é‡Œ API
    ZHIPUAI = "zhipuai" 
    VOLCANO = "volcano" # è±†åŒ…
    AWS = "aws"  # AWS Bedrock API
    HUGGINGFACE = "huggingface"

# ğŸ“Œ å®šä¹‰ AI æ¨¡å‹ï¼ˆModelï¼‰
class LLMModel(Enum):
    
    """AI æ¨¡å‹æšä¸¾ï¼ŒåŒ…æ‹¬ OpenAIã€DeepSeek å’Œ AWS"""
    # OpenAI æ¨¡å‹
    GPT_4O = ("gpt-4o", LLMSource.OPENAI)
    GPT_4O_MINI = ("gpt-4o-mini", LLMSource.OPENAI)
    GPT_35_TURBO = ("gpt-3.5-turbo", LLMSource.OPENAI)

    # QWEN æ¨¡å‹
    QWEN_TURBO = ("qwen-turbo", LLMSource.QWEN)
    QWEN_PLUS = ("qwen-plus", LLMSource.QWEN)
    QWEN_MAX= ("qwen-max", LLMSource.QWEN)
    
    # DeepSeek æ¨¡å‹
    DEEPSEEK_CHAT = ("deepseek-chat", LLMSource.DEEPSEEK)
    DEEPSEEK_REASONER = ("deepseek-reasoner", LLMSource.DEEPSEEK)
    DEEPSEEK_CODER = ("deepseek-coder", LLMSource.DEEPSEEK)
    
    
    # AWS Bedrock æ¨¡å‹
    # ref: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html
    # ref: https://github.com/langchain-ai/langchain-aws/issues/258 for â€œon-demand throughput isnâ€™t supported.â€
    AWS_LLAMA_3_2_3B = ("us.meta.llama3-2-3b-instruct-v1:0", LLMSource.AWS)
    AWS_CLAUDE_3_5_HAIKU = ("us.anthropic.claude-3-5-haiku-20241022-v1:0", LLMSource.AWS)

    # Huggingface æ¨¡å‹
    HF_MS_DIALOGPT_MEDIUM =  ("microsoft/DialoGPT-medium", LLMSource.HUGGINGFACE)  

    # zhipu æ¨¡å‹
    ZHIPUAI_GLM_4_PLUS =  ("glm-4-plus", LLMSource.ZHIPUAI) 

    # volcano
    VOL_DOUBAO_1_5_LITE = ("doubao-1.5-lite-32k-250115", LLMSource.VOLCANO) 
    VOL_DOUBAO_1_5_PRO = ("doubao-1.5-pro-32k-250115", LLMSource.VOLCANO) 

    def __init__(self, model_name, provider):
        """
        åˆå§‹åŒ–æ¨¡å‹æšä¸¾
        :param model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-4o"ï¼‰
        :param provider: ä¾›åº”å•†ï¼ˆSource.OPENAI / Source.DEEPSEEK / Source.AWSï¼‰
        """
        self.model_name = model_name  # å­˜å‚¨æ¨¡å‹åç§°
        self.provider = provider  # å­˜å‚¨ä¾›åº”å•†ä¿¡æ¯

# ğŸ“Œ AI èŠå¤©å®¢æˆ·ç«¯
class AIChatClient:
    """
    AI èŠå¤©å®¢æˆ·ç«¯ï¼Œæ”¯æŒ OpenAIã€DeepSeek å’Œ AWS Bedrockã€‚
    æ”¯æŒï¼š
    âœ… é€è¯æµå¼è¾“å‡º
    âœ… å®Œæ•´è¿”å›æ¨¡å¼
    """

    def __init__(self, model: LLMModel, temperature=0.7):
        """
        åˆå§‹åŒ– AI å®¢æˆ·ç«¯
        :param model: é€‰æ‹©çš„ AI æ¨¡å‹ï¼ˆå¦‚ Model.GPT_4O, Model.CLAUDE_V2ï¼‰
        """
        self.model = model  # å­˜å‚¨æ¨¡å‹
        self.provider = model.provider  # è·å–ä¾›åº”å•†ï¼ˆOpenAI / DeepSeek / AWSï¼‰
        self.temperature = temperature
        self.streaming_displace_delay_time = 0.05

        # ğŸ“Œ æ ¹æ®ä¾›åº”å•†é…ç½® API è®¿é—®
        if self.provider == LLMSource.OPENAI:
            self.api_key = os.getenv("openai_api_key")  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ OpenAI API å¯†é’¥
            self.base_url = None  # OpenAI é»˜è®¤ API ç«¯ç‚¹
            self.client = openai.OpenAI(api_key=self.api_key)

        if self.provider == LLMSource.QWEN:
            self.api_key = os.getenv("qwen_api_key")  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ OpenAI API å¯†é’¥
            self.base_url = "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"  # OpenAI é»˜è®¤ API ç«¯ç‚¹
            self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)

        elif self.provider == LLMSource.DEEPSEEK:
            self.api_key = os.getenv("deep_seek_api_key")  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ DeepSeek API å¯†é’¥
            self.base_url = "https://api.deepseek.com"  # DeepSeek API ç«¯ç‚¹
            self.client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)

        elif self.provider == LLMSource.ZHIPUAI:
            self.api_key = os.getenv("zhipuai_api_key")  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ DeepSeek API å¯†é’¥
            self.base_url = ""  # DeepSeek API ç«¯ç‚¹
            self.client = ZhipuAI(api_key=self.api_key)
           
        elif self.provider == LLMSource.VOLCANO:
            self.api_key = os.getenv("volcano_api_key")  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ DeepSeek API å¯†é’¥
            self.base_url = ""  # VOLCANO API ç«¯ç‚¹
            self.client  = Ark(api_key=os.getenv("volcano_api_key"))

        elif self.provider == LLMSource.AWS:
            self.aws_region = "us-west-2"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„ AWS åŒºåŸŸï¼ˆå¦‚ "us-east-1"ï¼‰
            self.client = boto3.client("bedrock-runtime", region_name=self.aws_region, 
                                       aws_access_key_id=os.getenv("aws_access_key_id"), aws_secret_access_key=os.getenv("aws_secret_access_key"))

        # ğŸ“Œ Hugging Face (`transformers`)
        elif self.provider == LLMSource.HUGGINGFACE:
            print(f"ğŸš€ åŠ è½½ Hugging Face æ¨¡å‹: {self.model.model_name}")
            path_huggingface_cache = os.getenv("huggingface_cache_location")
            print(f"ğŸ‘Œç¡®è®¤ Huggingface Cache ä½ç½®:{path_huggingface_cache}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model.model_name,  cache_dir=path_huggingface_cache, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(self.model.model_name,  cache_dir=path_huggingface_cache, trust_remote_code=True).to("cuda" if torch.cuda.is_available() else "cpu")
            print("\nğŸ’¬ æ¨¡å‹åŠ è½½å®Œæˆã€‚")     
        else:
            raise ValueError("âŒ æ— æ•ˆçš„ä¾›åº”å•†ï¼Œè¯·ä½¿ç”¨ Model æšä¸¾é€‰æ‹©æ¨¡å‹ï¼")

    def chat(self, prompt, stream=True):
        """
        å¤„ç† AI äº¤äº’ï¼Œæ”¯æŒæµå¼æ¨¡å¼å’Œéæµå¼æ¨¡å¼
        :param prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        :param stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”ï¼ˆTrue = é€è¯è¾“å‡º, False = ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç­”æ¡ˆï¼‰
        """
        if stream:
            self._chat_stream_word_by_word(prompt)
        else:
            return self._chat_non_stream(prompt)

    def is_chinese(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        return re.search(r'[\u4e00-\u9fff]', text) is not None
    

    def stream_response(self, response):
        """
        é€è¯æµå¼æ‰“å°å“åº”
        - ä¸­æ–‡ï¼šä¸åŠ ç©ºæ ¼
        - è‹±æ–‡ï¼šå•è¯ä¹‹é—´åŠ ç©ºæ ¼
        """
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                text = chunk.choices[0].delta.content  # è·å–æ–‡æœ¬
                
                if self.is_chinese(text):
                    print(text, end="", flush=True)  # ä¸­æ–‡å­—ç¬¦ç›´æ¥æ‹¼æ¥
                else:
                    words = text.split()  # æŒ‰ç©ºæ ¼æ‹†åˆ†å•è¯
                    for word in words:
                        print(word, end=" ", flush=True)  # è‹±æ–‡å•è¯ä¹‹é—´åŠ ç©ºæ ¼
                
                time.sleep(self.streaming_displace_delay_time)  # è½»å¾®å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿ AI æ‰“å­—æ•ˆæœ
        print()  # ç»“æŸæ¢è¡Œ

    def _chat_stream_word_by_word(self, prompt):
        """
        ğŸ“Œ é€è¯æµå¼å“åº”ï¼ˆé€‚ç”¨äº OpenAI / DeepSeek / AWS Bedrockï¼‰
        :param prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        """
        if self.provider in [LLMSource.OPENAI, LLMSource.DEEPSEEK, LLMSource.VOLCANO, LLMSource.ZHIPUAI, LLMSource.QWEN]:
            response = self.client.chat.completions.create(
                model=self.model.model_name,
                messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=True
            )
            print("\nğŸ’¬ AI é€è¯å“åº”ï¼š")
            self.stream_response(response)

        if self.provider in [LLMSource.HUGGINGFACE]:
            """
            ğŸ“Œ Hugging Face (`transformers`) ç”Ÿæˆæ–‡æœ¬
            """
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            output = self.model.generate(**inputs, max_length=150, pad_token_id=self.tokenizer.eos_token_id)
            response = self.tokenizer.decode(output[0], skip_special_tokens=True)
            print("\nğŸ’¬ AI é€è¯å“åº”ï¼š")
            for word in response:
                print(word, end=" ", flush=True)
                time.sleep(self.streaming_displace_delay_time)
            print()

        elif self.provider == LLMSource.AWS:
            if "claude" in self.model.model_name:

                body = {
                    "anthropic_version": "bedrock-2023-05-31",  # âœ… Claude 3.5 éœ€è¦æŒ‡å®šç‰ˆæœ¬
                    "messages": [{"role": "user", "content": prompt}],  # âœ… Claude 3.5 éœ€è¦ `messages`
                    "max_tokens": 1024,
                    "temperature": 0.7,
                    "top_p": 0.9
                }

                response = self.client.invoke_model(
                    modelId= self.model.model_name,  # âœ… Claude 3.5 Haiku
                    contentType="application/json",
                    accept="application/json",
                    body=json.dumps(body)
                )

                response_body = json.loads(response["body"].read().decode("utf-8"))
                
                # âœ… Claude 3.5 è¿”å›çš„æ•°æ®æ ¼å¼
                result =  response_body["content"][0]["text"]

                # body = json.dumps({
                #     "prompt": f"\n\nHuman: {prompt}\n\nAssistant:",
                #     "max_tokens_to_sample": 1024,
                #     "temperature": self.temperature
                # })


                # body = {
                #     "anthropic_version": "bedrock-2023-05-31",  # âœ… Claude 3.5 éœ€è¦æŒ‡å®šç‰ˆæœ¬
                #     "messages": [{"role": "user", "content": prompt}],  # âœ… Claude 3.5 éœ€è¦ `messages`
                #     "max_tokens": 1024,
                #     "temperature": self.temperature,
                #     "top_p": 0.9
                # }

                # response = self.client.invoke_model(
                #     modelId= self.model.model_name,  # âœ… Claude 3.5 Haiku
                #     contentType="application/json",
                #     accept="application/json",
                #     body=json.dumps(body)
                # )

                # response_body = json.loads(response["body"].read().decode("utf-8"))
                
                # # âœ… Claude 3.5 è¿”å›çš„æ•°æ®æ ¼å¼
                # result =  response_body["content"][0]["text"]

            elif "llama" in self.model.model_name:
                body = json.dumps({
                    "prompt": prompt,
                    "max_gen_len": 1024,
                    "temperature": self.temperature,
                    "top_p": 0.9
                })

                #
                response = self.client.invoke_model(
                    modelId=self.model.model_name,
                    contentType="application/json",
                    accept="application/json",
                    body=body
                )
                response_body = response["body"].read().decode("utf-8")
                response_json = json.loads(response_body)

                # ğŸ” **æ‰“å° AWS API è¿”å›çš„å®Œæ•´ JSON**
                # print("ğŸ” AWS API è¿”å›çš„å®Œæ•´ JSONï¼š", json.dumps(response_json, indent=2, ensure_ascii=False))

                # âœ… **è‡ªåŠ¨æ£€æµ‹æ­£ç¡®çš„ AI å“åº”å­—æ®µ**
                possible_keys = ["completion", "output", "generation", "text"]  # å…¼å®¹ä¸åŒæ¨¡å‹
                result = None
                for key in possible_keys:
                    if key in response_json:
                        result = response_json[key]
                        break  # æ‰¾åˆ°åŒ¹é…å­—æ®µå°±åœæ­¢å¾ªç¯

                if result is None:
                    raise ValueError("âŒ AI å“åº”æ ¼å¼ä¸åŒ¹é…ï¼è¯·æ£€æŸ¥ AWS è¿”å›çš„ JSON ç»“æ„ã€‚")
                #

            else:
                raise ValueError(f"âŒ ä¸æ”¯æŒçš„ AWS Bedrock æ¨¡å‹: {self.model.model_name}")



            words = result.split()
            print("\nğŸ’¬ AI é€è¯å“åº”ï¼š")
            
            for word in words:
                print(word, end=" ", flush=True)
                time.sleep(self.streaming_displace_delay_time)
            print()



    def _chat_non_stream(self, prompt):
        """
        ğŸ“Œ ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ AI å“åº”ï¼ˆéæµå¼æ¨¡å¼ï¼‰
        :param prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        :return: AI ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
        """
        if self.provider in [LLMSource.OPENAI, LLMSource.DEEPSEEK, LLMSource.VOLCANO, LLMSource.ZHIPUAI, LLMSource.QWEN]:
            response = self.client.chat.completions.create(
                model=self.model.model_name,
                messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚"},
                          {"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=False  # å…³é—­æµå¼æ¨¡å¼
            )
            return response.choices[0].message.content
     
                
        if self.provider in [LLMSource.HUGGINGFACE]:
            """
            ğŸ“Œ Hugging Face (`transformers`) ç”Ÿæˆæ–‡æœ¬
            """
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            output = self.model.generate(**inputs, max_length=150, pad_token_id=self.tokenizer.eos_token_id)
            response = self.tokenizer.decode(output[0], skip_special_tokens=True)
            return response


        elif self.provider == LLMSource.AWS:


            if "claude" in self.model.model_name:

                body = {
                    "anthropic_version": "bedrock-2023-05-31",  # âœ… Claude 3.5 éœ€è¦æŒ‡å®šç‰ˆæœ¬
                    "messages": [{"role": "user", "content": prompt}],  # âœ… Claude 3.5 éœ€è¦ `messages`
                    "max_tokens": 1024,
                    "temperature": 0.7,
                    "top_p": 0.9
                }

                response = self.client.invoke_model(
                    modelId= self.model.model_name,  # âœ… Claude 3.5 Haiku
                    contentType="application/json",
                    accept="application/json",
                    body=json.dumps(body)
                )

                response_body = json.loads(response["body"].read().decode("utf-8"))
                
                # âœ… Claude 3.5 è¿”å›çš„æ•°æ®æ ¼å¼
                result =  response_body["content"][0]["text"]


            elif "llama" in self.model.model_name:
                body = json.dumps({
                    "prompt": prompt,
                    "max_gen_len": 1024,
                    "temperature": self.temperature,
                    "top_p": 0.9
                })
                #
                response = self.client.invoke_model(
                    modelId=self.model.model_name,
                    contentType="application/json",
                    accept="application/json",
                    body=body
                )

                response_body = response["body"].read().decode("utf-8")
                response_json = json.loads(response_body)

                # ğŸ” **æ‰“å° AWS API è¿”å›çš„å®Œæ•´ JSON**
                # print("ğŸ” AWS API è¿”å›çš„å®Œæ•´ JSONï¼š", json.dumps(response_json, indent=2, ensure_ascii=False))

                # âœ… **è‡ªåŠ¨æ£€æµ‹æ­£ç¡®çš„ AI å“åº”å­—æ®µ**
                possible_keys = ["completion", "output", "generation", "text"]  # å…¼å®¹ä¸åŒæ¨¡å‹
                result = None
                for key in possible_keys:
                    if key in response_json:
                        result = response_json[key]
                        break  # æ‰¾åˆ°åŒ¹é…å­—æ®µå°±åœæ­¢å¾ªç¯

                if result is None:
                    raise ValueError("âŒ AI å“åº”æ ¼å¼ä¸åŒ¹é…ï¼è¯·æ£€æŸ¥ AWS è¿”å›çš„ JSON ç»“æ„ã€‚")
                #

            else:
                raise ValueError(f"âŒ ä¸æ”¯æŒçš„ AWS Bedrock æ¨¡å‹: {self.model.model_name}")

            return result
        
class Helper:

    def __init__(self):
        pass

    def run_ai_tests(self, user_input, models_to_test, k=1, output_file=""):
        """
        è¿è¡Œå¤šä¸ª AI æ¨¡å‹ï¼Œå¹¶é‡å¤ K æ¬¡è·å–å“åº”ï¼Œå­˜å‚¨åˆ° CSV æ–‡ä»¶
        :param user_input: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        :param models_to_test: éœ€è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
        :param k: æ¯ä¸ªæ¨¡å‹é‡å¤æ‰§è¡Œçš„æ¬¡æ•°
        :param output_file: ä¿å­˜ CSV çš„æ–‡ä»¶å
        :return: ç»“æœ Pandas DataFrame
        """
        # ğŸ“Œ å­˜å‚¨æ‰€æœ‰ç»“æœ
        results = []

        # ğŸ“Œ å­˜å‚¨å·²åˆå§‹åŒ–çš„æ¨¡å‹ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
        model_instances = {}

        # ğŸ“Œ éå†å¤šä¸ªæ¨¡å‹
        for model in models_to_test:
            print(f"\nğŸ” è¿è¡Œ {model.model_name} {k} æ¬¡...")

            # ğŸ”¹ ä»…åœ¨æ¨¡å‹æœªåˆå§‹åŒ–æ—¶åˆ›å»º AI å®¢æˆ·ç«¯
            if model not in model_instances:
                model_instances[model] = AIChatClient(model=model)

            # ğŸ”¹ è·å–å·²åˆå§‹åŒ–çš„æ¨¡å‹
            ai_client = model_instances[model]

            # ğŸ”¹ è¿è¡Œ K æ¬¡
            for i in range(1, k + 1):
                print(f"ğŸ”„ ç¬¬ {i} æ¬¡è°ƒç”¨ {model.model_name} ...")

                # ğŸ”¹ è·å– AI ç”Ÿæˆçš„å®Œæ•´å“åº”
                try:
                    full_response = ai_client.chat(user_input, stream=False)
                    print(f"ğŸ“ {model.model_name} ç¬¬ {i} æ¬¡è¿”å›: {full_response[:50]}...")  # ä»…æ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦
                except Exception as e:
                    full_response = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
                    print(full_response)

                # ğŸ“Œ è®°å½•ç»“æœ
                results.append({
                    "Iteration": i,  # ç¬¬å‡ æ¬¡è°ƒç”¨
                    "Model": model.model_name,
                    "Provider": model.provider.value,
                    "Response": full_response
                })

        # ğŸ“Œ ç”Ÿæˆ Pandas DataFrame
        df = pd.DataFrame(results)

        # ğŸ“Œ ä¿å­˜ DataFrame åˆ° CSV
        df.to_csv(output_file, index=False, encoding="utf-8-sig")

        print(f"\nâœ… æ‰€æœ‰æ¨¡å‹å“åº”å·²ä¿å­˜è‡³ {output_file}ï¼")

        return df
